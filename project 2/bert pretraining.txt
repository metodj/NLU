BERT pretrained files: https://drive.google.com/open?id=1oHDUwo-8hPuQG1onwCnVpzRfzdWvMRV0

To reproduce above model on Leonhard:
1) preprocess train dataset using BERT_Format_Files.py
2) install tensorflow gpu version 1.11 (does not work with 1.12 or 1.13)
3) clone Bert githup repository
4) download Bert model: uncased_L-12_H-768_A-12
5) run:
bsub -n 6 -W 4:00 -R "rusage[mem=2048, ngpus_excl_p=1]" python create_pretraining_data.py   
--input_file=data/BERT_pretraining_data.txt   
--output_file=data/tf_examples.tfrecord   
--vocab_file=models/uncased_L-12_H-768_A-12/vocab.txt   
--do_lower_case=True   
--max_seq_length=128   
--max_predictions_per_seq=20   
--masked_lm_prob=0.15   
--random_seed=12345   
--dupe_factor=5
6) run:
bsub -n 10 -W 4:00 -R "rusage[mem=3000, ngpus_excl_p=1]"  python run_pretraining.py   
--input_file=data/tf_examples.tfrecord   
--output_dir=data/pretraining_output   
--do_train=True   
--do_eval=True   
--bert_config_file=models/uncased_L-12_H-768_A-12/bert_config.json   
--init_checkpoint=models/uncased_L-12_H-768_A-12/bert_model.ckpt   
--train_batch_size=32   
--max_seq_length=128   
--max_predictions_per_seq=20   
--num_train_steps=10000   
--num_warmup_steps=10   
--learning_rate=2e-5
