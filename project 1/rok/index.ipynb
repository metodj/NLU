{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"index.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DYOLuNB7BQf8","colab_type":"code","outputId":"91861002-630f-454d-a014-7f94b6c18c56","executionInfo":{"status":"ok","timestamp":1554847492805,"user_tz":-120,"elapsed":23917,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","os.chdir(\"./gdrive/My Drive/NLU/Projects/project 1/rok/\")\n","\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","import warnings\n","\n","print(\"tf_version:\\t\" + tf.__version__)\n","warnings.filterwarnings(\"ignore\")\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","from model import Model\n","from load_embedding import load_embedding\n","import utils\n","import tf_utils\n","\n","logger = utils.Logger(\"./logs/\")\n","timer = utils.Timer()\n","\n","!pip install tensorboardcolab\n","from tensorboardcolab import *\n","tbc = TensorBoardColab()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n","tf_version:\t1.13.1\n","Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","https://12fcee36.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"m43Oi1wbAzyi","colab_type":"code","outputId":"2042b929-5127-4551-9d2f-e9d730cf09a4","executionInfo":{"status":"ok","timestamp":1554244666321,"user_tz":-120,"elapsed":18211,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":799}},"cell_type":"code","source":["# MODEL\n","tf.app.flags.DEFINE_string(\"EXPERIMENT\", \"A\", \"model selection (A, B, C)\")\n","tf.app.flags.DEFINE_string(\"MODE\", \"E\", \"mode (Experiment - E, Generation - G)\")\n","tf.app.flags.DEFINE_boolean(\"RESTORE\", False, \"mode (Experiment - E, Generation - G)\")\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# DIRECTORIES AND FILES\n","tf.app.flags.DEFINE_string(\"DATA_DIR\", \"./data/\", \"data directory\")\n","tf.app.flags.DEFINE_string(\"RESULTS_DIR\", \"./results/\", \"results directory\")\n","tf.app.flags.DEFINE_string(\"MODEL_DIR\", \"./model/\", \"saved model directory\")\n","tf.app.flags.DEFINE_string(\"WORD_EMBEDDINGS_FILE\", \"wordembeddings-dim100.word2vec\", \"word embedding file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_TRAIN_FILE\", \"sentences.train\", \"train file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_TEST_FILE\", \"sentences_test.txt\", \"test file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_EVAL_FILE\", \"sentences.eval\", \"evaluation file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_CONTINUATION_FILE\", \"sentences.continuation\", \"continuation file\")\n","\n","# LANGUAGE MODEL PARAMETERS\n","tf.app.flags.DEFINE_integer(\"EMBEDDING_DIM\", 100, \"word embedding dimension\")\n","tf.app.flags.DEFINE_integer(\"DOWN_STATE_DIM\", 512, \"down projection dimension\")\n","tf.app.flags.DEFINE_integer(\"STATE_DIM\", 512, \"rnn cell hidden state dimension\")\n","tf.app.flags.DEFINE_integer(\"VOCABULARY_SIZE\", 20000, \"vocabulary size\")\n","tf.app.flags.DEFINE_integer(\"SENT_DIM\", 30, \"train sentence length\")\n","tf.app.flags.DEFINE_integer(\"CONT_DIM\", 20, \"continuation max. sentence length\")\n","\n","# RNN PARAMETERS\n","tf.app.flags.DEFINE_integer(\"BATCH_SIZE\", 64, \"batch size\")\n","tf.app.flags.DEFINE_integer(\"NUM_EPOCHS\", 1, \"number of epochs for training\")\n","tf.app.flags.DEFINE_float(\"LEARNING_RATE\", 0.001, \"learning rate for rnn\")\n","tf.app.flags.DEFINE_float(\"MAX_GRAD_NORM\", 5.0, \"max. norm for gradient clipping\")\n","tf.app.flags.DEFINE_string('f', '', 'tensorflow bug')\n","\n","FLAGS = tf.app.flags.FLAGS\n","if FLAGS.EXPERIMENT == \"C\":\n","    FLAGS.STATE_DIM = 1024\n","tf_utils.print_flags(FLAGS)\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# LOAD VOCABULARY\n","with open(FLAGS.RESULTS_DIR + \"vocabulary.pkl\", \"rb\") as f:\n","    vocabulary, word_to_idx, idx_to_word = pickle.load(f)\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# RUN\n","tf.reset_default_graph()\n","tf.set_random_seed(12345)\n","np.random.seed(12345)\n","\n","model = Model(experiment=FLAGS.EXPERIMENT,\n","              mode=FLAGS.MODE,\n","              vocabulary_size=FLAGS.VOCABULARY_SIZE,\n","              embedding_dim=FLAGS.EMBEDDING_DIM,\n","              state_dim=FLAGS.STATE_DIM,\n","              down_state_dim=FLAGS.DOWN_STATE_DIM,\n","              sent_dim=FLAGS.SENT_DIM,\n","              cont_dim=FLAGS.CONT_DIM,\n","              initializer=tf.contrib.layers.xavier_initializer(),\n","              pad_idx=word_to_idx[\"<pad>\"],\n","              eos_idx=word_to_idx[\"<eos>\"],\n","              num_epochs=FLAGS.NUM_EPOCHS\n","              )\n","tf_utils.trainable_parameters()\n","\n","saver = tf.train.Saver()\n","timer.__enter__()\n","\n","with tf.Session() as session:\n","    writer = tbc.get_deep_writers(\"./\")\n","    writer.add_graph(session.graph)\n","    \n","    if FLAGS.MODE == \"E\":\n","        with tf.name_scope(\"experiment\"):\n","            if not FLAGS.RESTORE:\n","                session.run(tf.global_variables_initializer())\n","\n","                # LOAD EMBEDDING\n","                if FLAGS.EXPERIMENT == \"B\":\n","                    load_embedding(session, word_to_idx, model.embedding_weight,\n","                                   FLAGS.DATA_DIR + FLAGS.WORD_EMBEDDINGS_FILE, \n","                                   FLAGS.EMBEDDING_DIM, FLAGS.VOCABULARY_SIZE)\n","            else:\n","                saver.restore(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                              FLAGS.EXPERIMENT + \"/experiment\" + \n","                              FLAGS.EXPERIMENT + \".ckpt\")\n","                print(\"Model restored.\")\n","       \n","            # TRAINING\n","            summary_op = tf.summary.merge_all()\n","          \n","            session.run(model.iterator_op,\n","                        {model.sentences_file: FLAGS.RESULTS_DIR + \"X_train.ids\"})\n","\n","            batch_count = 0\n","            total_batch = 60000\n","            while True:\n","                try:\n","                    batch_loss, batch_perplexity, _, global_step, summary = session.run([model.loss, model.perplexity, \n","                                                                   model.optimize_op, model.global_step, summary_op])\n","                    writer.add_summary(summary, global_step)\n","                    epoch = 1\n","                    if batch_count % 100 == 0:\n","                        print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, FLAGS.NUM_EPOCHS,\n","                                                                                                          batch_count + 1,\n","                                                                                                          total_batch,\n","                                                                                                          batch_loss,\n","                                                                                                          batch_perplexity))\n","\n","                    batch_count += 1\n","                except tf.errors.OutOfRangeError:\n","                    break\n","            \n","            writer.flush()\n","            save_path = saver.save(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                                   FLAGS.EXPERIMENT + \"/experiment\" + \n","                                   FLAGS.EXPERIMENT + \".ckpt\")\n","            print(\"Model saved in path: %s\" % save_path)\n","\n","            # EVALUATION\n","            session.run(model.iterator_op, {model.sentences_file: FLAGS.RESULTS_DIR + \"X_eval.ids\"})\n","            eval_perplexities = np.array([], dtype=np.float32)\n","            batch_count = 0\n","            while True:\n","                try:\n","                    batch_perplexities = session.run(model.perplexities)\n","                    eval_perplexities = np.append(eval_perplexities, batch_perplexities)\n","                    batch_count += 1\n","                except tf.errors.OutOfRangeError:\n","                    break\n","            print(\"Evaluation finished.\")\n","\n","            with open(RESULTS_DIR + \"groupXX.perplexity\" + FLAGS.EXPERIMENT, \"w\") as f:\n","                for i in range(eval_perplexities.shape[0]):\n","                    f.write(\"%0.3f\" % eval_perplexities[i] + \"\\n\")\n","\n","    elif FLAGS.MODE == \"G\":\n","        with tf.name_scope(\"generation\"):\n","            saver.restore(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                          FLAGS.EXPERIMENT + \"/experiment\" + \n","                          FLAGS.EXPERIMENT + \".ckpt\")\n","            print(\"Model restored.\")\n","            \n","            session.run(model.iterator_op, {model.sentences_file: FLAGS.RESULTS_DIR + \"X_cont.ids\"})\n","\n","\n","            continuation_ids = []\n","            batch_count = 0\n","            while True:\n","                try:\n","                    batch_predictions = session.run(model.predictions)\n","                    continuation_ids.append(batch_predictions)\n","                    batch_count = batch_count + 1\n","\n","                    print(batch_count, end=\"\\r\")\n","                except tf.errors.OutOfRangeError:\n","                    break\n","\n","            continuation_ids = np.concatenate(continuation_ids, axis=0)\n","            print(continuation_ids.shape)\n","\n","            with open(FLAGS.RESULTS_DIR + \"groupXX.continuation\", \"w\") as f:\n","                for i in range(continuation_ids.shape[0]):\n","                    try:\n","                        eos_pos = continuation_ids[i, 1:].tolist().index(int(word_to_idx[\"<eos>\"]))\n","                    except:\n","                        eos_pos = 20\n","\n","                    gen_sent = \" \".join([idx_to_word[token_id] if idx < eos_pos else \"\" for idx, token_id in\n","                                         enumerate(continuation_ids[i, 1:].tolist())])\n","                    f.write(gen_sent + \"\\n\")\n","                \n","timer.__exit__()\n","tf_utils.delete_flags(FLAGS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Command-line Arguments:\n","EXPERIMENT                         : A\n","MODE                               : E\n","RESTORE                            : False\n","DATA_DIR                           : ./data/\n","RESULTS_DIR                        : ./results/\n","MODEL_DIR                          : ./model/\n","WORD_EMBEDDINGS_FILE               : wordembeddings-dim100.word2vec\n","SENTENCES_TRAIN_FILE               : sentences.train\n","SENTENCES_TEST_FILE                : sentences_test.txt\n","SENTENCES_EVAL_FILE                : sentences.eval\n","SENTENCES_CONTINUATION_FILE        : sentences.continuation\n","EMBEDDING_DIM                      : 100\n","DOWN_STATE_DIM                     : 512\n","STATE_DIM                          : 512\n","VOCABULARY_SIZE                    : 20000\n","SENT_DIM                           : 30\n","CONT_DIM                           : 20\n","BATCH_SIZE                         : 64\n","NUM_EPOCHS                         : 1\n","LEARNING_RATE                      : 0.001\n","MAX_GRAD_NORM                      : 5.0\n","\n","\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","output_weight:0                    :(512, 20000)\n","embedding_weight:0                 :(20000, 100)\n","lstm_cell/kernel:0                 :(612, 2048)\n","lstm_cell/bias:0                   :(2048,)   \n","num_parameters                     :13495424  \n","epoch: 1/1     batch:     1/60000     loss = 287.19       perp = 19998.91     \n","epoch: 1/1     batch:   101/60000     loss = 84.01        perp = 351.10       \n","epoch: 1/1     batch:   201/60000     loss = 82.50        perp = 218.58       \n","epoch: 1/1     batch:   301/60000     loss = 72.67        perp = 146.39       \n","epoch: 1/1     batch:   401/60000     loss = 60.39        perp = 146.90       \n","epoch: 1/1     batch:   501/60000     loss = 72.76        perp = 163.23       \n","epoch: 1/1     batch:   601/60000     loss = 72.10        perp = 179.82       \n","epoch: 1/1     batch:   701/60000     loss = 60.44        perp = 107.04       \n","epoch: 1/1     batch:   801/60000     loss = 59.88        perp = 120.77       \n","epoch: 1/1     batch:   901/60000     loss = 56.34        perp = 123.33       \n"],"name":"stdout"}]},{"metadata":{"id":"fyhTW2UrOeHC","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}