{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"index.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RzhzIxDJjlf_","colab_type":"code","outputId":"9cdbe767-6d24-4121-8c72-d5940f1a6861","executionInfo":{"status":"ok","timestamp":1553725785708,"user_tz":-60,"elapsed":502,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","os.chdir(\"./gdrive/My Drive/NLU/Projects/project 1/rok/\")\n","os.listdir()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['utils.py',\n"," 'load_embedding.py',\n"," 'tf_utils.py',\n"," 'data',\n"," 'docs',\n"," '.ipynb_checkpoints',\n"," '__pycache__',\n"," 'results',\n"," 'index.ipynb']"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"T8PvBnAPjezj","colab_type":"code","outputId":"46398e84-2f9e-4536-bff5-56f5b4cbc69e","executionInfo":{"status":"ok","timestamp":1553725809182,"user_tz":-60,"elapsed":19583,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["from load_embedding import load_embedding\n","import utils\n","import tf_utils\n","\n","import pickle\n","import numpy as np\n","from math import floor, ceil\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import tensorflow as tf\n","print(\"tf_version:\\t\" + tf.__version__)\n","\n","!pip install tensorboardcolab\n","from tensorboardcolab import TensorBoardColab\n","\n","tbc = TensorBoardColab()\n","logger = utils.Logger(\"./logs/\")\n","timer = utils.Timer()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tf_version:\t1.13.1\n","Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","http://2eeeff66.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"Wkxte3jljezr","colab_type":"code","colab":{}},"cell_type":"code","source":["#------------------------------------------------------------------------------------------------------------------------------#\n","# DIRECTORIES\n","DATA_DIR = \"./data/\"\n","RESULTS_DIR = \"./results/\"\n","WORD_EMBEDDINGS_FILE = \"wordembeddings-dim100.word2vec\"\n","SENTENCES_TRAIN_FILE = \"sentences.train\"\n","SENTENCES_TEST_FILE = \"sentences_test.txt\"\n","SENTENCES_EVAL_FILE = \"sentences.eval\"\n","SENTENCES_CONTINUATION_FILE = \"sentences.continuation\"\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# LANGUAGE MODEL PARAMETERS\n","EMBEDDING_DIM = 100\n","STATE_DIM = 512\n","VOCABULARY_SIZE = 20000\n","SENT_DIM = 30\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# RNN PARAMETERS\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.001\n","MAX_GRAD_NORM = 5.0\n","NUM_EPOCHS = 1\n","KEEP_PROBS = 0.5\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# LOAD DATA\n","LOAD_DATA = True\n","LOAD_EMBEDDING = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-vBqvnCcjezv","colab_type":"code","outputId":"99f5ae78-8c1d-4c08-867d-dd3e6510b4eb","executionInfo":{"status":"ok","timestamp":1553725839853,"user_tz":-60,"elapsed":938,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"cell_type":"code","source":["if LOAD_DATA:\n","    with open(RESULTS_DIR + \"vocabulary.pkl\", \"rb\") as f:\n","        vocabulary, word_to_idx, idx_to_word = pickle.load(f)\n","        \n","    X_train = np.load(RESULTS_DIR + \"X_train.npy\")\n","    X_test = np.load(RESULTS_DIR + \"X_test.npy\")\n","    X_eval = np.load(RESULTS_DIR + \"X_eval.npy\")\n","\n","else:\n","    vocabulary, word_to_idx, idx_to_word = utils.create_vocabulary(DATA_DIR + SENTENCES_TRAIN_FILE, VOCABULARY_SIZE)\n","    X_train = utils.create_dataset(DATA_DIR + SENTENCES_TRAIN_FILE, word_to_idx)\n","    X_test = utils.create_dataset(DATA_DIR + SENTENCES_TEST_FILE, word_to_idx)\n","    X_eval = utils.create_dataset(DATA_DIR + SENTENCES_EVAL_FILE, word_to_idx)\n","    \n","    with open(RESULTS_DIR + \"vocabulary.pkl\", \"wb\") as f:\n","        pickle.dump((vocabulary, word_to_idx, idx_to_word), f)\n","    \n","    with open(RESULTS_DIR + \"X_train.ids\", \"w\") as f:\n","        for i in range(X_train.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_train[i, :]]) + \"\\n\")\n","     \n","    with open(RESULTS_DIR + \"X_test.ids\", \"w\") as f:\n","        for i in range(X_test.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_test[i, :]]) + \"\\n\")\n","    \n","    with open(RESULTS_DIR + \"X_eval.ids\", \"w\") as f:\n","        for i in range(X_eval.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_eval[i, :]]) + \"\\n\")\n","    \n","    np.save(RESULTS_DIR + \"X_train.npy\", X_train)\n","    np.save(RESULTS_DIR + \"X_test.npy\", X_test)\n","    np.save(RESULTS_DIR + \"X_eval.npy\", X_eval)\n","\n","num_train = X_train.shape[0]\n","num_test = X_test.shape[0]\n","num_eval = X_eval.shape[0]\n","    \n","logger.append(\"vocabulary:\", len(vocabulary))\n","logger.append(\"X_train:\", X_train.shape)\n","logger.append(\"X_test:\", X_test.shape)\n","logger.append(\"X_eval:\", X_eval.shape)\n","logger.append(\"<bos> idx\", word_to_idx[\"<bos>\"])\n","logger.append(\"<eos> idx\", word_to_idx[\"<eos>\"])\n","logger.append(\"<pad> idx\", word_to_idx[\"<pad>\"])\n","logger.append(\"<unk> idx\", word_to_idx[\"<unk>\"])\n","logger.append(\"DATA LOADED.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["vocabulary:                             20000          \n","X_train:                                (1969833, 30)  \n","X_test:                                 (10000, 30)    \n","X_eval:                                 (9846, 30)     \n","<bos> idx                               178            \n","<eos> idx                               179            \n","<pad> idx                               180            \n","<unk> idx                               181            \n","DATA LOADED.                            \n"],"name":"stdout"}]},{"metadata":{"id":"Fzl5glNbjezz","colab_type":"code","outputId":"a262e33f-dd5b-4fb2-e102-07ed8596c6b6","executionInfo":{"status":"ok","timestamp":1553723501628,"user_tz":-60,"elapsed":921560,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":986}},"cell_type":"code","source":["timer.__enter__()\n","tf.reset_default_graph()\n","\n","with tf.name_scope(\"initialization\"):\n","    tf.set_random_seed(12345)\n","    np.random.seed(12345)\n","    initializer = tf.contrib.layers.xavier_initializer()\n","\n","with tf.name_scope(\"input\"):\n","    with tf.name_scope(\"train_dataset\"):\n","        sentences_train_file_name = tf.placeholder(tf.string)\n","        training_dataset = tf.data.TextLineDataset(sentences_train_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n","        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n","        X_batch, y_batch = iterator.get_next()\n","        training_init_op = iterator.make_initializer(training_dataset)\n","\n","    with tf.name_scope(\"evaluation_dataset\"):\n","        sentences_eval_file_name = tf.placeholder(tf.string)\n","        eval_dataset = tf.data.TextLineDataset(sentences_eval_file_name).map(tf_utils.parse_ids_file).batch(BATCH_SIZE)\n","        eval_iterator = tf.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n","        X_eval_batch, y_eval_batch = eval_iterator.get_next()\n","        eval_init_op = eval_iterator.make_initializer(eval_dataset)\n","\n","\n","with tf.name_scope(\"weights\"):\n","    with tf.name_scope(\"output_weight\"):\n","        output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n","                                        initializer=initializer, trainable=True) # 512x20000\n","    with tf.name_scope(\"embedding_weight\"):\n","        if not LOAD_EMBEDDING:\n","            embedding_weight = tf.get_variable(\"embedding_weight\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], \n","                                               initializer=initializer, trainable=True) # 20000x100\n","        else:\n","            embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[], trainable=False)  # 20000x100\n","    \n","with tf.name_scope(\"lstm_initialization\"):\n","    LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n","    with tf.name_scope(\"dropout\"):\n","        LSTM = tf.nn.rnn_cell.DropoutWrapper(LSTM, input_keep_prob=KEEP_PROBS, output_keep_prob=KEEP_PROBS, \n","                                             state_keep_prob=KEEP_PROBS)\n","        \n","    batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","\n","\n","\n","with tf.name_scope(\"training\"):\n","  \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n","  \n","    losses = []\n","    probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_t = X_batch_embedded[:, t, :]  # 64x100\n","        y_t = y_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x512\n","            logits = tf.matmul(lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n","            losses.append(loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            probability_t = tf.math.exp(-loss_t)\n","            probabilities.append(probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        losses = tf.stack(losses)  # 29x64 \n","        loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n","\n","        perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","\n","with tf.name_scope(\"optimize\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n","    optimize_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"evaluation\"):\n","    batch_size = tf.shape(X_eval_batch)[0]\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","    \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_eval_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_eval_batch)  # 64x29x100\n","  \n","    eval_losses = []\n","    eval_probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_eval_t = X_eval_batch_embedded[:, t, :]  # 64x100\n","        y_eval_t = y_eval_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            eval_lstm_output, (state_c, state_h) = LSTM(inputs=X_eval_t, state=(state_c, state_h))  # 64x512\n","            eval_logits = tf.matmul(eval_lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            eval_loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_eval_t, logits=eval_logits)  # 64x1\n","            eval_losses.append(eval_loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            eval_probability_t = tf.math.exp(-eval_loss_t)\n","            eval_probabilities.append(eval_probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        eval_losses = tf.stack(eval_losses)  # 29x64 \n","        eval_loss = tf.reduce_mean(tf.reduce_sum(eval_losses,axis=1))  # 29x1 -> 1x1\n","\n","        eval_perplexity = tf.exp(tf.reduce_mean(eval_losses, axis=0)) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","  \n","  \n","with tf.Session() as session:\n","    # Initialize variables\n","    session.run(tf.global_variables_initializer())\n","    tf_utils.trainable_parameters()\n","    \n","    # Load data\n","    session.run(training_init_op, {sentences_train_file_name: RESULTS_DIR + \"X_train.ids\"})\n","    \n","    # Load embedding\n","    if LOAD_EMBEDDING:\n","        load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n","\n","        \n","    # Training\n","    epoch = 0\n","    batch_count = 0\n","    total_batch = num_train / BATCH_SIZE\n","    while True:\n","\n","        try:\n","            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n","            epoch = floor(batch_count / total_batch) + 1\n","            \n","            if batch_count % 100 == 0:\n","                print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, NUM_EPOCHS, \n","                                                        batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n","            \n","            batch_count += 1\n","            \n","            if batch_count > 5000:\n","                break\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","    # Evaluation     \n","    session.run(eval_init_op, {sentences_eval_file_name: RESULTS_DIR + \"X_eval.ids\"})\n","    \n","    batch_count = 0\n","    total_batch = num_eval / BATCH_SIZE\n","    \n","    eval_perplexities = np.array([], dtype=np.float32)\n","    while True:\n","\n","        try:\n","            batch_perplexity = session.run(eval_perplexity)\n","            eval_perplexities = np.append(eval_perplexities, batch_perplexity)\n","            batch_count += 1\n","               \n","        except tf.errors.OutOfRangeError:\n","            break\n","    \n","    \n","timer.__exit__()\n","\n","with open(RESULTS_DIR + \"groupXX.perplexityA\", \"w\") as f:\n","    for i in range(num_eval):\n","        f.write(str(eval_perplexities[i]) + \"\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["output_weight:0               (512, 20000)\n","embedding_weight:0            (20000, 100)\n","basic_lstm_cell/kernel:0      (612, 2048)\n","basic_lstm_cell/bias:0           (2048,)\n","num_parameters                  13495424\n","epoch: 1/1     batch:     1/30779     loss = 633.83       perp = 20001.68     \n","epoch: 1/1     batch:   101/30779     loss = 184.66       perp = 73.69        \n","epoch: 1/1     batch:   201/30779     loss = 182.44       perp = 53.26        \n","epoch: 1/1     batch:   301/30779     loss = 163.06       perp = 40.61        \n","epoch: 1/1     batch:   401/30779     loss = 137.13       perp = 24.38        \n","epoch: 1/1     batch:   501/30779     loss = 166.35       perp = 49.18        \n","epoch: 1/1     batch:   601/30779     loss = 165.96       perp = 53.30        \n","epoch: 1/1     batch:   701/30779     loss = 140.62       perp = 27.89        \n","epoch: 1/1     batch:   801/30779     loss = 139.33       perp = 26.84        \n","epoch: 1/1     batch:   901/30779     loss = 130.82       perp = 27.19        \n","epoch: 1/1     batch:  1001/30779     loss = 146.07       perp = 30.82        \n","epoch: 1/1     batch:  1101/30779     loss = 144.26       perp = 24.35        \n","epoch: 1/1     batch:  1201/30779     loss = 158.67       perp = 42.22        \n","epoch: 1/1     batch:  1301/30779     loss = 144.71       perp = 19.34        \n","epoch: 1/1     batch:  1401/30779     loss = 140.53       perp = 26.86        \n","epoch: 1/1     batch:  1501/30779     loss = 145.14       perp = 34.89        \n","epoch: 1/1     batch:  1601/30779     loss = 138.93       perp = 25.80        \n","epoch: 1/1     batch:  1701/30779     loss = 146.52       perp = 23.67        \n","epoch: 1/1     batch:  1801/30779     loss = 144.99       perp = 25.00        \n","epoch: 1/1     batch:  1901/30779     loss = 125.48       perp = 17.52        \n","epoch: 1/1     batch:  2001/30779     loss = 150.70       perp = 29.27        \n","epoch: 1/1     batch:  2101/30779     loss = 133.78       perp = 22.34        \n","epoch: 1/1     batch:  2201/30779     loss = 129.74       perp = 26.70        \n","epoch: 1/1     batch:  2301/30779     loss = 141.87       perp = 27.97        \n","epoch: 1/1     batch:  2401/30779     loss = 124.82       perp = 18.45        \n","epoch: 1/1     batch:  2501/30779     loss = 136.23       perp = 24.72        \n","epoch: 1/1     batch:  2601/30779     loss = 122.26       perp = 16.43        \n","epoch: 1/1     batch:  2701/30779     loss = 108.39       perp = 10.02        \n","epoch: 1/1     batch:  2801/30779     loss = 143.92       perp = 39.52        \n","epoch: 1/1     batch:  2901/30779     loss = 147.82       perp = 46.33        \n","epoch: 1/1     batch:  3001/30779     loss = 119.57       perp = 22.22        \n","epoch: 1/1     batch:  3101/30779     loss = 121.87       perp = 17.22        \n","epoch: 1/1     batch:  3201/30779     loss = 116.75       perp = 15.26        \n","epoch: 1/1     batch:  3301/30779     loss = 122.03       perp = 28.92        \n","epoch: 1/1     batch:  3401/30779     loss = 140.39       perp = 22.49        \n","epoch: 1/1     batch:  3501/30779     loss = 131.28       perp = 14.73        \n","epoch: 1/1     batch:  3601/30779     loss = 126.50       perp = 19.19        \n","epoch: 1/1     batch:  3701/30779     loss = 124.83       perp = 18.09        \n","epoch: 1/1     batch:  3801/30779     loss = 148.79       perp = 52.91        \n","epoch: 1/1     batch:  3901/30779     loss = 127.33       perp = 20.34        \n","epoch: 1/1     batch:  4001/30779     loss = 144.24       perp = 32.02        \n","epoch: 1/1     batch:  4101/30779     loss = 148.49       perp = 30.97        \n","epoch: 1/1     batch:  4201/30779     loss = 121.33       perp = 17.16        \n","epoch: 1/1     batch:  4301/30779     loss = 128.08       perp = 21.55        \n","epoch: 1/1     batch:  4401/30779     loss = 137.29       perp = 24.18        \n","epoch: 1/1     batch:  4501/30779     loss = 115.52       perp = 10.49        \n","epoch: 1/1     batch:  4601/30779     loss = 131.34       perp = 20.33        \n","epoch: 1/1     batch:  4701/30779     loss = 129.09       perp = 17.40        \n","epoch: 1/1     batch:  4801/30779     loss = 120.85       perp = 11.79        \n","epoch: 1/1     batch:  4901/30779     loss = 132.64       perp = 26.67        \n","epoch: 1/1     batch:  5001/30779     loss = 146.73       perp = 29.32        \n","Elapsed: 920.6701648235321s\n"],"name":"stdout"}]},{"metadata":{"id":"c7eMJdUTjez6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":714},"outputId":"f3dc10d8-4506-49b9-a370-1046427356c4","executionInfo":{"status":"ok","timestamp":1553726138040,"user_tz":-60,"elapsed":227546,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}}},"cell_type":"code","source":["timer.__enter__()\n","tf.reset_default_graph()\n","\n","with tf.name_scope(\"initialization\"):\n","    LOAD_EMBEDDING = True\n","    tf.set_random_seed(12345)\n","    np.random.seed(12345)\n","    initializer = tf.contrib.layers.xavier_initializer()\n","\n","with tf.name_scope(\"input\"):\n","    with tf.name_scope(\"train_dataset\"):\n","        sentences_train_file_name = tf.placeholder(tf.string)\n","        training_dataset = tf.data.TextLineDataset(sentences_train_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n","        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n","        X_batch, y_batch = iterator.get_next()\n","        training_init_op = iterator.make_initializer(training_dataset)\n","\n","    with tf.name_scope(\"evaluation_dataset\"):\n","        sentences_eval_file_name = tf.placeholder(tf.string)\n","        eval_dataset = tf.data.TextLineDataset(sentences_eval_file_name).map(tf_utils.parse_ids_file).batch(BATCH_SIZE)\n","        eval_iterator = tf.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n","        X_eval_batch, y_eval_batch = eval_iterator.get_next()\n","        eval_init_op = eval_iterator.make_initializer(eval_dataset)\n","\n","\n","with tf.name_scope(\"weights\"):\n","    with tf.name_scope(\"output_weight\"):\n","        output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n","                                        initializer=initializer, trainable=True) # 512x20000\n","    with tf.name_scope(\"embedding_weight\"):\n","        if not LOAD_EMBEDDING:\n","            embedding_weight = tf.get_variable(\"embedding_weight\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], \n","                                               initializer=initializer, trainable=True) # 20000x100\n","        else:\n","            embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[], trainable=False)  # 20000x100\n","    \n","with tf.name_scope(\"lstm_initialization\"):\n","    LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n","    with tf.name_scope(\"dropout\"):\n","        LSTM = tf.nn.rnn_cell.DropoutWrapper(LSTM, input_keep_prob=KEEP_PROBS, output_keep_prob=KEEP_PROBS, \n","                                             state_keep_prob=KEEP_PROBS)\n","        \n","    batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","\n","\n","\n","with tf.name_scope(\"training\"):\n","  \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n","  \n","    losses = []\n","    probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_t = X_batch_embedded[:, t, :]  # 64x100\n","        y_t = y_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x512\n","            logits = tf.matmul(lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n","            losses.append(loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            probability_t = tf.math.exp(-loss_t)\n","            probabilities.append(probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        losses = tf.stack(losses)  # 29x64 \n","        loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n","\n","        perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","\n","with tf.name_scope(\"optimize\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n","    optimize_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"evaluation\"):\n","    batch_size = tf.shape(X_eval_batch)[0]\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","    \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_eval_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_eval_batch)  # 64x29x100\n","  \n","    eval_losses = []\n","    eval_probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_eval_t = X_eval_batch_embedded[:, t, :]  # 64x100\n","        y_eval_t = y_eval_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            eval_lstm_output, (state_c, state_h) = LSTM(inputs=X_eval_t, state=(state_c, state_h))  # 64x512\n","            eval_logits = tf.matmul(eval_lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            eval_loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_eval_t, logits=eval_logits)  # 64x1\n","            eval_losses.append(eval_loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            eval_probability_t = tf.math.exp(-eval_loss_t)\n","            eval_probabilities.append(eval_probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        eval_losses = tf.stack(eval_losses)  # 29x64 \n","        eval_loss = tf.reduce_mean(tf.reduce_sum(eval_losses,axis=1))  # 29x1 -> 1x1\n","\n","        eval_perplexity = tf.exp(tf.reduce_mean(eval_losses, axis=0)) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","  \n","  \n","with tf.Session() as session:\n","    # Initialize variables\n","    session.run(tf.global_variables_initializer())\n","    tf_utils.trainable_parameters()\n","    \n","    # Load data\n","    session.run(training_init_op, {sentences_train_file_name: RESULTS_DIR + \"X_train.ids\"})\n","    \n","    # Load embedding\n","    if LOAD_EMBEDDING:\n","        load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n","\n","        \n","    # Training\n","    epoch = 0\n","    batch_count = 0\n","    total_batch = num_train / BATCH_SIZE\n","    while True:\n","\n","        try:\n","            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n","            epoch = floor(batch_count / total_batch) + 1\n","            \n","            if batch_count % 100 == 0:\n","                print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, NUM_EPOCHS, \n","                                                        batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n","            \n","            batch_count += 1\n","            \n","            if batch_count > 1002:\n","                break\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","    # Evaluation     \n","    session.run(eval_init_op, {sentences_eval_file_name: RESULTS_DIR + \"X_eval.ids\"})\n","    \n","    batch_count = 0\n","    total_batch = num_eval / BATCH_SIZE\n","    \n","    eval_perplexities = np.array([], dtype=np.float32)\n","    while True:\n","\n","        try:\n","            batch_perplexity = session.run(eval_perplexity)\n","            eval_perplexities = np.append(eval_perplexities, batch_perplexity)\n","            batch_count += 1\n","               \n","        except tf.errors.OutOfRangeError:\n","            break\n","    \n","    \n","timer.__exit__()\n","\n","with open(RESULTS_DIR + \"groupXX.perplexityB\", \"w\") as f:\n","    for i in range(num_eval):\n","        f.write(str(eval_perplexities[i]) + \"\\n\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-5-9f062bd1baee>:38: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","output_weight:0               (512, 20000)\n","basic_lstm_cell/kernel:0      (612, 2048)\n","basic_lstm_cell/bias:0        (2048,)   \n","num_parameters                11495424  \n","Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n","<bos> not in embedding file\n","<eos> not in embedding file\n","<pad> not in embedding file\n","<unk> not in embedding file\n","19996 words out of 20000 could be loaded\n","epoch: 1/1     batch:     1/30779     loss = 631.37       perp = 19253.23     \n","epoch: 1/1     batch:   101/30779     loss = 156.91       perp = 37.70        \n","epoch: 1/1     batch:   201/30779     loss = 165.01       perp = 35.99        \n","epoch: 1/1     batch:   301/30779     loss = 145.15       perp = 23.99        \n","epoch: 1/1     batch:   401/30779     loss = 124.28       perp = 16.74        \n","epoch: 1/1     batch:   501/30779     loss = 153.23       perp = 33.51        \n","epoch: 1/1     batch:   601/30779     loss = 152.84       perp = 36.76        \n","epoch: 1/1     batch:   701/30779     loss = 129.34       perp = 21.56        \n","epoch: 1/1     batch:   801/30779     loss = 131.29       perp = 20.88        \n","epoch: 1/1     batch:   901/30779     loss = 122.98       perp = 20.64        \n","epoch: 1/1     batch:  1001/30779     loss = 137.22       perp = 25.63        \n","Elapsed: 226.58593130111694s\n"],"name":"stdout"}]},{"metadata":{"id":"diPHm4VyR-21","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}