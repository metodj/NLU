{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roksi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_version:\t1.10.0\n"
     ]
    }
   ],
   "source": [
    "from load_embedding import load_embedding\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "word_embeddings = \"wordembeddings-dim100.word2vec\"\n",
    "\n",
    "sentences_train_file = \"sentences.train\"\n",
    "sentences_eval_file = \"sentences.eval\"\n",
    "sentences_continuation_file = \"sentences.continuation\"\n",
    "sentences_test_file = \"sentences_test.txt\"\n",
    "\n",
    "print(\"tf_version:\\t\" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCABULARY_SIZE = 2000\n",
    "\n",
    "vocabulary, word_to_idx, idx_to_word = create_vocabulary(data_dir + sentences_train_file, VOCABULARY_SIZE)\n",
    "X_train = create_dataset(data_dir + sentences_train_file, word_to_idx)\n",
    "X_test = create_dataset(data_dir + sentences_test_file, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n",
      "<bos> not in embedding file\n",
      "<eos> not in embedding file\n",
      "<pad> not in embedding file\n",
      "<unk> not in embedding file\n",
      "1996 words out of 20000 could be loaded\n"
     ]
    }
   ],
   "source": [
    "emb = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    load_embedding(session, word_to_idx, emb, data_dir + word_embeddings, EMBEDDING_DIM, VOCABULARY_SIZE)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "model = models.KeyedVectors.load_word2vec_format(data_dir + word_embeddings, binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0588529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape\n",
    "\n",
    "model.wv.vectors.mean(axis=1).mean()\n",
    "model.wv.vectors.std(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# PARAMETERS\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "STATE_DIM = 512\n",
    "VOCABULARY_SIZE = 20000\n",
    "\n",
    "sent_dim = X_train.shape[1]\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "# Initializer\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Parameters\n",
    "W = tf.get_variable(\"W\", shape=[STATE_DIM, VOCABULARY_SIZE], initializer=initializer, trainable=True)\n",
    "E = tf.get_variable(\"E\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], initializer=initializer, trainable=True)\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.int32, (None, sent_dim))\n",
    "\n",
    "# LSTM initialization\n",
    "LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n",
    "state_c, state_h = LSTM.zero_state(batch_size=BATCH_SIZE, dtype=tf.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# RNN forward pass\n",
    "for t in range(0, sent_dim - 1):\n",
    "    X_t = X[:, t]\n",
    "    y_t = X[:, t+1]\n",
    "    \n",
    "    X_t = tf.one_hot(X_t, depth = VOCABULARY_SIZE)\n",
    "    E_t = tf.matmul(X_t, E)\n",
    "\n",
    "    output, (state_c, state_h) = LSTM(inputs=E_t, state=(state_c, state_h))\n",
    "    logits = tf.matmul(output, W)\n",
    "    \n",
    "    loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)\n",
    "    \n",
    "    losses.append(loss_t)\n",
    "    \n",
    "losses = tf.reduce_sum(tf.stack(losses),axis=1)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n",
    "optimize_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# SESSION\n",
    "np.random.seed(12345)\n",
    "\n",
    "batches_per_epoch = 20\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    session.run(init)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        print('epoch\\t%4d' % epoch + 1)\n",
    "        \n",
    "        for idx in range(batches_per_epoch):\n",
    "            batch_loss, _ = session.run([loss, optimize_op],\n",
    "                                    feed_dict={X: X_test[(idx*BATCH_SIZE):((idx+1)*BATCH_SIZE)]}\n",
    "                                    )\n",
    "            epoch_loss += batch_loss\n",
    "            print('\\tbatch %4d\\t%.2f' % (idx + 1, batch_loss))\n",
    "        \n",
    "#         if epoch + 1 % 2 == 0:\n",
    "        print('epoch\\t%4d\\t%.2f' % (epoch + 1, epoch_loss / batches_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "STATE_DIM = 512\n",
    "VOCABULARY_SIZE = 20000\n",
    "\n",
    "sent_dim = X_train.shape[1]\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "batch_per_epoch = floor(num_test / BATCH_SIZE)\n",
    "\n",
    "# Session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initializer\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Parameters\n",
    "W = tf.get_variable(\"W\", shape=[STATE_DIM, VOCABULARY_SIZE], initializer=initializer, trainable=True)\n",
    "E = tf.get_variable(\"E\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], initializer=initializer, trainable=True)\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.int32, (None, sent_dim))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X).batch(batch_size).repeat()\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "X_batch = iterator.get_next()\n",
    "\n",
    "# LSTM initialization\n",
    "LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n",
    "state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# RNN forward pass\n",
    "for t in range(0, 5):\n",
    "    X_t = X_batch[:, t]\n",
    "    y_t = X_batch[:, t+1]\n",
    "    \n",
    "    X_t = tf.one_hot(X_t, depth = VOCABULARY_SIZE)\n",
    "    E_t = tf.matmul(X_t, E)\n",
    "\n",
    "    output, (state_c, state_h) = LSTM(inputs=E_t, state=(state_c, state_h))\n",
    "    logits = tf.matmul(output, W)\n",
    "    \n",
    "    loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)\n",
    "    \n",
    "    losses.append(loss_t)\n",
    "    \n",
    "losses = tf.stack(losses)\n",
    "losses = tf.reduce_sum(losses,axis=1)\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n",
    "optimize_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "# SESSION\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(init)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for _ in range(batch_per_epoch):\n",
    "        batch_loss, _ = sess.run([loss, train_op, loss])\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        \n",
    "#     train_loss, _ = session.run([loss, optimize_op],\n",
    "#                             feed_dict={X: X_test[0:batch_size]}\n",
    "#                             )\n",
    "\n",
    "    if epoch + 1 % 1 == 0:\n",
    "        print('Epoch %04d> training loss: %.2f' % (epoch, total_loss))\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
