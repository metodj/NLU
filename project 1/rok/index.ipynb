{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"index.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RzhzIxDJjlf_","colab_type":"code","outputId":"6dd19d4c-fb5a-4d77-bed0-4bd1f45fe8ab","executionInfo":{"status":"ok","timestamp":1553962991615,"user_tz":-60,"elapsed":569,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","os.chdir(\"./gdrive/My Drive/NLU/Projects/project 1/rok/\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"T8PvBnAPjezj","colab_type":"code","outputId":"9212c48d-22b4-4e3b-a2f6-ea8c3285bf22","executionInfo":{"status":"ok","timestamp":1553963013539,"user_tz":-60,"elapsed":19406,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"cell_type":"code","source":["from load_embedding import load_embedding\n","import utils\n","import tf_utils\n","\n","import pickle\n","import numpy as np\n","from math import floor, ceil\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import tensorflow as tf\n","print(\"tf_version:\\t\" + tf.__version__)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n","\n","!pip install tensorboardcolab\n","from tensorboardcolab import TensorBoardColab\n","\n","tbc = TensorBoardColab()\n","logger = utils.Logger(\"./logs/\")\n","timer = utils.Timer()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tf_version:\t1.13.1\n","Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","https://d678854e.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"Wkxte3jljezr","colab_type":"code","colab":{}},"cell_type":"code","source":["#------------------------------------------------------------------------------------------------------------------------------#\n","# DIRECTORIES\n","DATA_DIR = \"./data/\"\n","RESULTS_DIR = \"./results/\"\n","WORD_EMBEDDINGS_FILE = \"wordembeddings-dim100.word2vec\"\n","SENTENCES_TRAIN_FILE = \"sentences.train\"\n","SENTENCES_TEST_FILE = \"sentences_test.txt\"\n","SENTENCES_EVAL_FILE = \"sentences.eval\"\n","SENTENCES_CONTINUATION_FILE = \"sentences.continuation\"\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# LANGUAGE MODEL PARAMETERS\n","EMBEDDING_DIM = 100\n","STATE_DIM = 512\n","VOCABULARY_SIZE = 20000\n","SENT_DIM = 30\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# RNN PARAMETERS\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.001\n","MAX_GRAD_NORM = 5.0\n","NUM_EPOCHS = 1\n","KEEP_PROBS = 0.5\n","\n","#------------------------------------------------------------------------------------------------------------------------------#\n","# LOAD DATA\n","LOAD_DATA = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-vBqvnCcjezv","colab_type":"code","outputId":"db34008a-7009-4153-8836-79ee0789baee","executionInfo":{"status":"ok","timestamp":1553963013765,"user_tz":-60,"elapsed":5513,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"cell_type":"code","source":["if LOAD_DATA:\n","    with open(RESULTS_DIR + \"vocabulary.pkl\", \"rb\") as f:\n","        vocabulary, word_to_idx, idx_to_word = pickle.load(f)\n","        \n","    X_train = np.load(RESULTS_DIR + \"X_train.npy\")\n","    X_test = np.load(RESULTS_DIR + \"X_test.npy\")\n","    X_eval = np.load(RESULTS_DIR + \"X_eval.npy\")\n","\n","else:\n","    vocabulary, word_to_idx, idx_to_word = utils.create_vocabulary(DATA_DIR + SENTENCES_TRAIN_FILE, VOCABULARY_SIZE)\n","    X_train = utils.create_dataset(DATA_DIR + SENTENCES_TRAIN_FILE, word_to_idx)\n","    X_test = utils.create_dataset(DATA_DIR + SENTENCES_TEST_FILE, word_to_idx)\n","    X_eval = utils.create_dataset(DATA_DIR + SENTENCES_EVAL_FILE, word_to_idx)\n","    \n","    with open(RESULTS_DIR + \"vocabulary.pkl\", \"wb\") as f:\n","        pickle.dump((vocabulary, word_to_idx, idx_to_word), f)\n","    \n","    with open(RESULTS_DIR + \"X_train.ids\", \"w\") as f:\n","        for i in range(X_train.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_train[i, :]]) + \"\\n\")\n","     \n","    with open(RESULTS_DIR + \"X_test.ids\", \"w\") as f:\n","        for i in range(X_test.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_test[i, :]]) + \"\\n\")\n","    \n","    with open(RESULTS_DIR + \"X_eval.ids\", \"w\") as f:\n","        for i in range(X_eval.shape[0]):\n","            f.write(\" \".join([str(x) for x in X_eval[i, :]]) + \"\\n\")\n","    \n","    np.save(RESULTS_DIR + \"X_train.npy\", X_train)\n","    np.save(RESULTS_DIR + \"X_test.npy\", X_test)\n","    np.save(RESULTS_DIR + \"X_eval.npy\", X_eval)\n","\n","num_train = X_train.shape[0]\n","num_test = X_test.shape[0]\n","num_eval = X_eval.shape[0]\n","    \n","logger.append(\"vocabulary:\", len(vocabulary))\n","logger.append(\"X_train:\", X_train.shape)\n","logger.append(\"X_test:\", X_test.shape)\n","logger.append(\"X_eval:\", X_eval.shape)\n","logger.append(\"<bos> idx\", word_to_idx[\"<bos>\"])\n","logger.append(\"<eos> idx\", word_to_idx[\"<eos>\"])\n","logger.append(\"<pad> idx\", word_to_idx[\"<pad>\"])\n","logger.append(\"<unk> idx\", word_to_idx[\"<unk>\"])\n","logger.append(\"DATA LOADED.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["vocabulary:                             20000          \n","X_train:                                (1969833, 30)  \n","X_test:                                 (10000, 30)    \n","X_eval:                                 (9846, 30)     \n","<bos> idx                               178            \n","<eos> idx                               179            \n","<pad> idx                               180            \n","<unk> idx                               181            \n","DATA LOADED.                            \n"],"name":"stdout"}]},{"metadata":{"id":"Fzl5glNbjezz","colab_type":"code","outputId":"19721c34-7ee3-48b7-893c-2002c6847008","executionInfo":{"status":"ok","timestamp":1553951164560,"user_tz":-60,"elapsed":24467,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":1078}},"cell_type":"code","source":["# EXPERIMENT A\n","print(\"EXPERIMENT A\")\n","timer.__enter__()\n","tf.reset_default_graph()\n","\n","with tf.name_scope(\"initialization\"):\n","    tf.set_random_seed(12345)\n","    np.random.seed(12345)\n","    initializer = tf.contrib.layers.xavier_initializer()\n","\n","with tf.name_scope(\"input\"):\n","    with tf.name_scope(\"train_dataset\"):\n","        sentences_train_file_name = tf.placeholder(tf.string)\n","        training_dataset = tf.data.TextLineDataset(sentences_train_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n","        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n","        X_batch, y_batch = iterator.get_next()\n","        training_init_op = iterator.make_initializer(training_dataset)\n","\n","    with tf.name_scope(\"evaluation_dataset\"):\n","        sentences_eval_file_name = tf.placeholder(tf.string)\n","        eval_dataset = tf.data.TextLineDataset(sentences_eval_file_name).map(tf_utils.parse_ids_file).batch(BATCH_SIZE)\n","        eval_iterator = tf.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n","        X_eval_batch, y_eval_batch = eval_iterator.get_next()\n","        eval_init_op = eval_iterator.make_initializer(eval_dataset)\n","\n","\n","with tf.name_scope(\"weights\"):\n","    output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n","                                    initializer=initializer, trainable=True) # 512x20000\n","    embedding_weight = tf.get_variable(\"embedding_weight\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], \n","                                           initializer=initializer, trainable=True) # 20000x100\n","\n","    \n","with tf.name_scope(\"lstm_initialization\"):\n","    LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n","#     with tf.name_scope(\"dropout\"):\n","#         LSTM = tf.nn.rnn_cell.DropoutWrapper(LSTM, input_keep_prob=KEEP_PROBS, output_keep_prob=KEEP_PROBS, \n","#                                              state_keep_prob=KEEP_PROBS)\n","    batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","\n","\n","\n","with tf.name_scope(\"training\"):\n","  \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n","  \n","    losses = []\n","    probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_t = X_batch_embedded[:, t, :]  # 64x100\n","        y_t = y_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x512\n","            logits = tf.matmul(lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n","            losses.append(loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            probability_t = tf.math.exp(-loss_t)\n","            probabilities.append(probability_t)\n","    \n","    print(\"LSTM\", LSTM.graph)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        losses = tf.stack(losses)  # 29x64 \n","        loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n","\n","        perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","\n","    tf.summary.scalar(\"perplexity\", perplexity)\n","    tf.summary.scalar(\"loss\", loss)\n","            \n","        \n","with tf.name_scope(\"optimize\"):\n","    global_step = tf.Variable(1, name=\"global_step\", trainable=False)\n","    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n","    optimize_op = optimizer.minimize(loss, global_step=global_step)\n","\n","with tf.name_scope(\"evaluation\"):\n","    batch_size = tf.shape(X_eval_batch)[0]\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","    \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_eval_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_eval_batch)  # 64x29x100\n","  \n","    eval_losses = []\n","    eval_probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_eval_t = X_eval_batch_embedded[:, t, :]  # 64x100\n","        y_eval_t = y_eval_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            eval_lstm_output, (state_c, state_h) = LSTM(inputs=X_eval_t, state=(state_c, state_h))  # 64x512\n","            eval_logits = tf.matmul(eval_lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            eval_loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_eval_t, logits=eval_logits)  # 64x1\n","            eval_losses.append(eval_loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            eval_probability_t = tf.math.exp(-eval_loss_t)\n","            eval_probabilities.append(eval_probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        eval_losses = tf.stack(eval_losses)  # 29x64 \n","        eval_loss = tf.reduce_mean(tf.reduce_sum(eval_losses,axis=1))  # 29x1 -> 1x1\n","\n","        eval_perplexity = tf.exp(tf.reduce_mean(eval_losses, axis=0)) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","  \n","  \n","  \n","with tf.Session() as session:\n","    print(\"session\", session.graph)\n","    # Tensorboard\n","    train_writer = tbc.get_deep_writers(\"rnn_lstm/train\")\n","    train_writer.add_graph(session.graph)\n","  \n","    summaries_merged = tf.summary.merge_all()\n","\n","    # Initialize variables\n","    session.run(tf.global_variables_initializer())\n","    tf_utils.trainable_parameters()\n","    print(tf.global_variables())\n","    # Load data\n","    session.run(training_init_op, {sentences_train_file_name: RESULTS_DIR + \"X_train.ids\"})\n","    print(session.graph.collections)\n","\n","    summary = session.run(summaries_merged)\n","        \n","    # Training\n","    step = tf.train.global_step(session, global_step)\n","    epoch = 0\n","    batch_count = 0\n","    total_batch = num_train / BATCH_SIZE\n","    while True:\n","\n","        try:\n","            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n","            epoch = floor(batch_count / total_batch) + 1\n","            \n","            if batch_count % 1 == 0:\n","                print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, NUM_EPOCHS, \n","                                                        batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n","            \n","            \n","            train_writer.add_summary(summary, step)\n","\n","            batch_count += 1\n","\n","            if batch_count > 50:\n","                break\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","    train_writer.flush()\n","\n","    # Evaluation     \n","    session.run(eval_init_op, {sentences_eval_file_name: RESULTS_DIR + \"X_eval.ids\"})\n","    \n","    batch_count = 0\n","    total_batch = num_eval / BATCH_SIZE\n","    \n","    eval_perplexities = np.array([], dtype=np.float32)\n","    while True:\n","\n","        try:\n","            batch_perplexity = session.run(eval_perplexity)\n","            eval_perplexities = np.append(eval_perplexities, batch_perplexity)\n","            batch_count += 1\n","               \n","        except tf.errors.OutOfRangeError:\n","            break\n","    \n","    \n","\n","# with open(RESULTS_DIR + \"groupXX.perplexityA\", \"w\") as f:\n","#     for i in range(num_eval):\n","#         f.write(\"%0.3f\" % eval_perplexities[i] + \"\\n\")\n","        \n","timer.__exit__()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["EXPERIMENT A\n","LSTM <tensorflow.python.framework.ops.Graph object at 0x7f96675e7748>\n","session <tensorflow.python.framework.ops.Graph object at 0x7f96675e7748>\n","output_weight:0               (512, 20000)\n","embedding_weight:0            (20000, 100)\n","basic_lstm_cell/kernel:0      (612, 2048)\n","basic_lstm_cell/bias:0        (2048,)   \n","num_parameters                13495424  \n","[<tf.Variable 'output_weight:0' shape=(512, 20000) dtype=float32_ref>, <tf.Variable 'embedding_weight:0' shape=(20000, 100) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/kernel:0' shape=(612, 2048) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/bias:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'optimize/global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'optimize/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'optimize/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'output_weight/Adam:0' shape=(512, 20000) dtype=float32_ref>, <tf.Variable 'output_weight/Adam_1:0' shape=(512, 20000) dtype=float32_ref>, <tf.Variable 'embedding_weight/Adam:0' shape=(20000, 100) dtype=float32_ref>, <tf.Variable 'embedding_weight/Adam_1:0' shape=(20000, 100) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/kernel/Adam:0' shape=(612, 2048) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/kernel/Adam_1:0' shape=(612, 2048) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/bias/Adam:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'basic_lstm_cell/bias/Adam_1:0' shape=(2048,) dtype=float32_ref>]\n","[('__varscope',), 'iterators', ('__variable_store',), 'variables', 'trainable_variables', 'update_ops', 'summaries', 'train_op']\n","epoch: 1/1     batch:     1/30779     loss = 633.82       perp = 19998.67     \n","epoch: 1/1     batch:     2/30779     loss = 633.38       perp = 19861.75     \n","epoch: 1/1     batch:     3/30779     loss = 632.46       perp = 19577.81     \n","epoch: 1/1     batch:     4/30779     loss = 628.92       perp = 18534.01     \n","epoch: 1/1     batch:     5/30779     loss = 607.32       perp = 13307.49     \n","epoch: 1/1     batch:     6/30779     loss = 540.36       perp = 4776.59      \n","epoch: 1/1     batch:     7/30779     loss = 479.12       perp = 2164.93      \n","epoch: 1/1     batch:     8/30779     loss = 421.73       perp = 941.50       \n","epoch: 1/1     batch:     9/30779     loss = 362.96       perp = 485.33       \n","epoch: 1/1     batch:    10/30779     loss = 319.33       perp = 313.18       \n","epoch: 1/1     batch:    11/30779     loss = 283.55       perp = 256.19       \n","epoch: 1/1     batch:    12/30779     loss = 243.60       perp = 167.46       \n","epoch: 1/1     batch:    13/30779     loss = 248.46       perp = 319.70       \n","epoch: 1/1     batch:    14/30779     loss = 261.29       perp = 300.59       \n","epoch: 1/1     batch:    15/30779     loss = 240.44       perp = 359.19       \n","epoch: 1/1     batch:    16/30779     loss = 236.12       perp = 254.81       \n","epoch: 1/1     batch:    17/30779     loss = 235.99       perp = 309.99       \n","epoch: 1/1     batch:    18/30779     loss = 236.44       perp = 526.07       \n","epoch: 1/1     batch:    19/30779     loss = 241.56       perp = 405.84       \n","epoch: 1/1     batch:    20/30779     loss = 226.52       perp = 212.53       \n","epoch: 1/1     batch:    21/30779     loss = 249.01       perp = 350.23       \n","epoch: 1/1     batch:    22/30779     loss = 218.62       perp = 370.62       \n","epoch: 1/1     batch:    23/30779     loss = 251.68       perp = 534.47       \n","epoch: 1/1     batch:    24/30779     loss = 258.71       perp = 891.16       \n","epoch: 1/1     batch:    25/30779     loss = 244.55       perp = 347.00       \n","epoch: 1/1     batch:    26/30779     loss = 243.15       perp = 294.76       \n","epoch: 1/1     batch:    27/30779     loss = 247.26       perp = 261.66       \n","epoch: 1/1     batch:    28/30779     loss = 228.25       perp = 211.12       \n","epoch: 1/1     batch:    29/30779     loss = 233.18       perp = 155.57       \n","epoch: 1/1     batch:    30/30779     loss = 247.75       perp = 300.08       \n","epoch: 1/1     batch:    31/30779     loss = 217.09       perp = 262.78       \n","epoch: 1/1     batch:    32/30779     loss = 242.82       perp = 453.20       \n","epoch: 1/1     batch:    33/30779     loss = 215.47       perp = 211.25       \n","epoch: 1/1     batch:    34/30779     loss = 275.77       perp = 477.33       \n","epoch: 1/1     batch:    35/30779     loss = 249.05       perp = 530.73       \n","epoch: 1/1     batch:    36/30779     loss = 258.83       perp = 430.07       \n","epoch: 1/1     batch:    37/30779     loss = 225.34       perp = 135.26       \n","epoch: 1/1     batch:    38/30779     loss = 226.16       perp = 185.10       \n","epoch: 1/1     batch:    39/30779     loss = 224.35       perp = 251.50       \n","epoch: 1/1     batch:    40/30779     loss = 214.82       perp = 150.19       \n","epoch: 1/1     batch:    41/30779     loss = 222.95       perp = 291.64       \n","epoch: 1/1     batch:    42/30779     loss = 204.80       perp = 222.65       \n","epoch: 1/1     batch:    43/30779     loss = 244.39       perp = 463.77       \n","epoch: 1/1     batch:    44/30779     loss = 241.48       perp = 703.85       \n","epoch: 1/1     batch:    45/30779     loss = 240.86       perp = 313.18       \n","epoch: 1/1     batch:    46/30779     loss = 253.41       perp = 348.80       \n","epoch: 1/1     batch:    47/30779     loss = 236.54       perp = 413.69       \n","epoch: 1/1     batch:    48/30779     loss = 256.61       perp = 1089.22      \n","epoch: 1/1     batch:    49/30779     loss = 230.81       perp = 217.93       \n","epoch: 1/1     batch:    50/30779     loss = 251.54       perp = 271.79       \n","epoch: 1/1     batch:    51/30779     loss = 239.55       perp = 267.99       \n","Elapsed: 23.52480936050415s\n"],"name":"stdout"}]},{"metadata":{"id":"c7eMJdUTjez6","colab_type":"code","outputId":"245a67c2-661f-44ca-a4a3-e4156b2b7a61","executionInfo":{"status":"ok","timestamp":1553896040381,"user_tz":-60,"elapsed":182746,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"cell_type":"code","source":["# EXPERIMENT B\n","print(\"EXPERIMENT B\")\n","timer.__enter__()\n","tf.reset_default_graph()\n","\n","with tf.name_scope(\"initialization\"):\n","    LOAD_EMBEDDING = True\n","    tf.set_random_seed(12345)\n","    np.random.seed(12345)\n","    initializer = tf.contrib.layers.xavier_initializer()\n","\n","with tf.name_scope(\"input\"):\n","    with tf.name_scope(\"train_dataset\"):\n","        sentences_train_file_name = tf.placeholder(tf.string)\n","        training_dataset = tf.data.TextLineDataset(sentences_train_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n","        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n","        X_batch, y_batch = iterator.get_next()\n","        training_init_op = iterator.make_initializer(training_dataset)\n","\n","    with tf.name_scope(\"evaluation_dataset\"):\n","        sentences_eval_file_name = tf.placeholder(tf.string)\n","        eval_dataset = tf.data.TextLineDataset(sentences_eval_file_name).map(tf_utils.parse_ids_file).batch(BATCH_SIZE)\n","        eval_iterator = tf.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n","        X_eval_batch, y_eval_batch = eval_iterator.get_next()\n","        eval_init_op = eval_iterator.make_initializer(eval_dataset)\n","\n","\n","with tf.name_scope(\"weights\"):\n","        output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n","                                        initializer=initializer, trainable=True) # 512x20000\n","\n","        embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[], trainable=False)  # 20000x100\n","    \n","with tf.name_scope(\"lstm_initialization\"):\n","    LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n","#     with tf.name_scope(\"dropout\"):\n","#         LSTM = tf.nn.rnn_cell.DropoutWrapper(LSTM, input_keep_prob=KEEP_PROBS, output_keep_prob=KEEP_PROBS, \n","#                                              state_keep_prob=KEEP_PROBS)\n","        \n","    batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","\n","\n","\n","with tf.name_scope(\"training\"):\n","  \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n","  \n","    losses = []\n","    probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_t = X_batch_embedded[:, t, :]  # 64x100\n","        y_t = y_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x512\n","            logits = tf.matmul(lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n","            losses.append(loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            probability_t = tf.math.exp(-loss_t)\n","            probabilities.append(probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        losses = tf.stack(losses)  # 29x64 \n","        loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n","\n","        perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","\n","with tf.name_scope(\"optimize\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n","    optimize_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"evaluation\"):\n","    batch_size = tf.shape(X_eval_batch)[0]\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","    \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_eval_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_eval_batch)  # 64x29x100\n","  \n","    eval_losses = []\n","    eval_probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_eval_t = X_eval_batch_embedded[:, t, :]  # 64x100\n","        y_eval_t = y_eval_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            eval_lstm_output, (state_c, state_h) = LSTM(inputs=X_eval_t, state=(state_c, state_h))  # 64x512\n","            eval_logits = tf.matmul(eval_lstm_output, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            eval_loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_eval_t, logits=eval_logits)  # 64x1\n","            eval_losses.append(eval_loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            eval_probability_t = tf.math.exp(-eval_loss_t)\n","            eval_probabilities.append(eval_probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        eval_losses = tf.stack(eval_losses)  # 29x64 \n","        eval_loss = tf.reduce_mean(tf.reduce_sum(eval_losses,axis=1))  # 29x1 -> 1x1\n","\n","        eval_perplexity = tf.exp(tf.reduce_mean(eval_losses, axis=0)) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","  \n","  \n","with tf.Session() as session:\n","    # Initialize variables\n","    session.run(tf.global_variables_initializer())\n","    tf_utils.trainable_parameters()\n","    \n","    # Load data\n","    session.run(training_init_op, {sentences_train_file_name: RESULTS_DIR + \"X_train.ids\"})\n","    \n","    # Load embedding\n","    load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n","\n","        \n","    # Training\n","    epoch = 0\n","    batch_count = 0\n","    total_batch = num_train / BATCH_SIZE\n","    while True:\n","\n","        try:\n","            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n","            epoch = floor(batch_count / total_batch) + 1\n","            \n","            if batch_count % 100 == 0:\n","                print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, NUM_EPOCHS, \n","                                                        batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n","            \n","            batch_count += 1\n","            \n","            if batch_count > 1002:\n","                break\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","    # Evaluation     \n","    session.run(eval_init_op, {sentences_eval_file_name: RESULTS_DIR + \"X_eval.ids\"})\n","    \n","    batch_count = 0\n","    total_batch = num_eval / BATCH_SIZE\n","    \n","    eval_perplexities = np.array([], dtype=np.float32)\n","    while True:\n","\n","        try:\n","            batch_perplexity = session.run(eval_perplexity)\n","            eval_perplexities = np.append(eval_perplexities, batch_perplexity)\n","            batch_count += 1\n","               \n","        except tf.errors.OutOfRangeError:\n","            break\n","    \n","    \n","timer.__exit__()\n","\n","with open(RESULTS_DIR + \"groupXX.perplexityB\", \"w\") as f:\n","    for i in range(num_eval):\n","        f.write(\"%0.3f\" % eval_perplexities[i] + \"\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["EXPERIMENT B\n","output_weight:0               (512, 20000)\n","basic_lstm_cell/kernel:0      (612, 2048)\n","basic_lstm_cell/bias:0        (2048,)   \n","num_parameters                11495424  \n","Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n","<bos> not in embedding file\n","<eos> not in embedding file\n","<pad> not in embedding file\n","<unk> not in embedding file\n","19996 words out of 20000 could be loaded\n","epoch: 1/1     batch:     1/30779     loss = 637.57       perp = 21212.82     \n","epoch: 1/1     batch:   101/30779     loss = 147.07       perp = 29.00        \n","epoch: 1/1     batch:   201/30779     loss = 152.99       perp = 26.63        \n","epoch: 1/1     batch:   301/30779     loss = 132.83       perp = 17.27        \n","epoch: 1/1     batch:   401/30779     loss = 114.41       perp = 12.76        \n","epoch: 1/1     batch:   501/30779     loss = 140.09       perp = 23.08        \n","epoch: 1/1     batch:   601/30779     loss = 139.18       perp = 24.94        \n","epoch: 1/1     batch:   701/30779     loss = 118.01       perp = 14.81        \n","epoch: 1/1     batch:   801/30779     loss = 119.13       perp = 14.72        \n","epoch: 1/1     batch:   901/30779     loss = 113.06       perp = 15.51        \n","epoch: 1/1     batch:  1001/30779     loss = 124.64       perp = 17.22        \n","Elapsed: 181.47050714492798s\n"],"name":"stdout"}]},{"metadata":{"id":"diPHm4VyR-21","colab_type":"code","outputId":"8967ec1f-8503-4449-9418-4a966289bded","executionInfo":{"status":"ok","timestamp":1553897397944,"user_tz":-60,"elapsed":225095,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":420}},"cell_type":"code","source":["STATE_DIM = 1024\n","DOWN_STATE_DIM = 512\n","\n","# EXPERIMENT C\n","print(\"EXPERIMENT C\")\n","timer.__enter__()\n","tf.reset_default_graph()\n","\n","with tf.name_scope(\"initialization\"):\n","    LOAD_EMBEDDING = True\n","    tf.set_random_seed(12345)\n","    np.random.seed(12345)\n","    initializer = tf.contrib.layers.xavier_initializer()\n","\n","with tf.name_scope(\"input\"):\n","    with tf.name_scope(\"train_dataset\"):\n","        sentences_train_file_name = tf.placeholder(tf.string)\n","        training_dataset = tf.data.TextLineDataset(sentences_train_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n","        iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n","        X_batch, y_batch = iterator.get_next()\n","        training_init_op = iterator.make_initializer(training_dataset)\n","\n","    with tf.name_scope(\"evaluation_dataset\"):\n","        sentences_eval_file_name = tf.placeholder(tf.string)\n","        eval_dataset = tf.data.TextLineDataset(sentences_eval_file_name).map(tf_utils.parse_ids_file).batch(BATCH_SIZE)\n","        eval_iterator = tf.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n","        X_eval_batch, y_eval_batch = eval_iterator.get_next()\n","        eval_init_op = eval_iterator.make_initializer(eval_dataset)\n","\n","\n","with tf.name_scope(\"weights\"):\n","        output_weight = tf.get_variable(\"output_weight\", shape=[DOWN_STATE_DIM, VOCABULARY_SIZE], \n","                                        initializer=initializer, trainable=True) # 512x20000\n","        down_project_weight = tf.get_variable(\"down_project_weight\", shape = [STATE_DIM, DOWN_STATE_DIM], \n","                                              initializer=initializer, trainable=True)  # 1024x512\n","        embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[], trainable=False)  # 20000x100\n","    \n","    \n","with tf.name_scope(\"lstm_initialization\"):\n","    LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n","    batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x1024\n","\n","with tf.name_scope(\"training\"):\n","  \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n","  \n","    losses = []\n","    probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_t = X_batch_embedded[:, t, :]  # 64x100\n","        y_t = y_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x1024\n","            \n","            down_project_logits = tf.matmul(lstm_output, down_project_weight)  # 64x512\n","            logits = tf.matmul(down_project_logits, output_weight)  # 64x20000\n","            \n","\n","        with tf.name_scope(\"loss\"):\n","            loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n","            losses.append(loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            probability_t = tf.math.exp(-loss_t)\n","            probabilities.append(probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        losses = tf.stack(losses)  # 29x64 \n","        loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n","\n","        perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","\n","with tf.name_scope(\"optimize\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n","    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n","    optimize_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"evaluation\"):\n","    batch_size = tf.shape(X_eval_batch)[0]\n","    state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n","    \n","    with tf.name_scope(\"embedding_lookup\"):\n","        X_eval_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_eval_batch)  # 64x29x100\n","  \n","    eval_losses = []\n","    eval_probabilities = []\n","\n","    for t in range(0, SENT_DIM - 1):\n","        X_eval_t = X_eval_batch_embedded[:, t, :]  # 64x100\n","        y_eval_t = y_eval_batch[:, t]  # 64x1\n","        \n","        with tf.name_scope(\"lstm_fp\"):\n","            eval_lstm_output, (state_c, state_h) = LSTM(inputs=X_eval_t, state=(state_c, state_h))  # 64x1024\n","            eval_down_project_logits = tf.matmul(eval_lstm_output, down_project_weight)  # 64x512\n","            eval_logits = tf.matmul(eval_down_project_logits, output_weight)  # 64x20000\n","\n","        with tf.name_scope(\"loss\"):\n","            eval_loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_eval_t, logits=eval_logits)  # 64x1\n","            eval_losses.append(eval_loss_t)\n","        \n","        with tf.name_scope(\"probability\"):\n","            eval_probability_t = tf.math.exp(-eval_loss_t)\n","            eval_probabilities.append(eval_probability_t)\n","    \n","    with tf.name_scope(\"aggregate_losses\"):\n","        eval_losses = tf.stack(eval_losses)  # 29x64 \n","        eval_loss = tf.reduce_mean(tf.reduce_sum(eval_losses,axis=1))  # 29x1 -> 1x1\n","\n","        eval_perplexity = tf.exp(tf.reduce_mean(eval_losses, axis=0)) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n","  \n","  \n","with tf.Session() as session:\n","    # Initialize variables\n","    session.run(tf.global_variables_initializer())\n","    tf_utils.trainable_parameters()\n","    \n","    # Load data\n","    session.run(training_init_op, {sentences_train_file_name: RESULTS_DIR + \"X_train.ids\"})\n","    \n","    # Load embedding\n","    load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n","\n","        \n","    # Training\n","    epoch = 0\n","    batch_count = 0\n","    total_batch = num_train / BATCH_SIZE\n","    while True:\n","\n","        try:\n","            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n","            epoch = floor(batch_count / total_batch) + 1\n","            \n","            if batch_count % 100 == 0:\n","                print(\"epoch: {}/{:<6}batch: {:>5}/{:<10}loss = {:<13.2f}perp = {:<13.2f}\".format(epoch, NUM_EPOCHS, \n","                                                        batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n","            \n","            batch_count += 1\n","            \n","            if batch_count > 1002:\n","                break\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","    # Evaluation     \n","    session.run(eval_init_op, {sentences_eval_file_name: RESULTS_DIR + \"X_eval.ids\"})\n","    \n","    batch_count = 0\n","    total_batch = num_eval / BATCH_SIZE\n","    \n","    eval_perplexities = np.array([], dtype=np.float32)\n","    while True:\n","\n","        try:\n","            batch_perplexity = session.run(eval_perplexity)\n","            eval_perplexities = np.append(eval_perplexities, batch_perplexity)\n","            batch_count += 1\n","               \n","        except tf.errors.OutOfRangeError:\n","            break\n","    \n","    \n","timer.__exit__()\n","\n","with open(RESULTS_DIR + \"groupXX.perplexityC\", \"w\") as f:\n","    for i in range(num_eval):\n","        f.write(\"%0.3f\" % eval_perplexities[i] + \"\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["EXPERIMENT C\n","output_weight:0               (512, 20000)\n","down_project_weight:0         (1024, 512)\n","basic_lstm_cell/kernel:0      (1124, 4096)\n","basic_lstm_cell/bias:0        (4096,)   \n","num_parameters                15372288  \n","Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n","<bos> not in embedding file\n","<eos> not in embedding file\n","<pad> not in embedding file\n","<unk> not in embedding file\n","19996 words out of 20000 could be loaded\n","epoch: 1/1     batch:     1/30779     loss = 633.98       perp = 20049.46     \n","epoch: 1/1     batch:   101/30779     loss = 144.10       perp = 26.35        \n","epoch: 1/1     batch:   201/30779     loss = 151.58       perp = 26.74        \n","epoch: 1/1     batch:   301/30779     loss = 131.61       perp = 16.74        \n","epoch: 1/1     batch:   401/30779     loss = 113.45       perp = 12.48        \n","epoch: 1/1     batch:   501/30779     loss = 139.64       perp = 23.21        \n","epoch: 1/1     batch:   601/30779     loss = 138.18       perp = 25.56        \n","epoch: 1/1     batch:   701/30779     loss = 116.29       perp = 14.06        \n","epoch: 1/1     batch:   801/30779     loss = 117.64       perp = 14.28        \n","epoch: 1/1     batch:   901/30779     loss = 113.46       perp = 16.07        \n","epoch: 1/1     batch:  1001/30779     loss = 123.72       perp = 16.15        \n","Elapsed: 223.92614555358887s\n"],"name":"stdout"}]},{"metadata":{"id":"VNJnPNSiXAma","colab_type":"code","outputId":"48efdbbd-aa76-4325-8fc0-a030e47a59d1","executionInfo":{"status":"error","timestamp":1553945409201,"user_tz":-60,"elapsed":1548,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":2833}},"cell_type":"code","source":["tf.reset_default_graph()\n","from models import ModelA\n","\n","model = ModelA(vocabulary_size=VOCABULARY_SIZE, embedding_dim=EMBEDDING_DIM, state_dim=STATE_DIM, sent_dim=SENT_DIM,\n","               initializer=tf.contrib.layers.xavier_initializer(), pad_idx=180)\n","\n","\n","init_op = tf.global_variables_initializer()\n","\n","with tf.Session() as session:\n","    session.run(init_op)\n","    print(tf.trainable_variables())\n","\n","    session.run(model.set_iterator(), {model.sentences_file: RESULTS_DIR + \"X_train.ids\"})\n","\n","    a = session.run(model.forward_pass(model.X_batch, model.y_batch))\n","    print(a)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[<tf.Variable 'output_weight:0' shape=(512, 20000) dtype=float32_ref>, <tf.Variable 'embedding_weight:0' shape=(20000, 100) dtype=float32_ref>]\n"],"name":"stdout"},{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value basic_lstm_cell/bias\n\t [[{{node basic_lstm_cell/bias/read}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-eef802166585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRESULTS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"X_train.ids\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value basic_lstm_cell/bias\n\t [[node basic_lstm_cell/bias/read (defined at /content/gdrive/My Drive/NLU/Projects/project 1/rok/models.py:106) ]]\n\nCaused by op 'basic_lstm_cell/bias/read', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-eef802166585>\", line 16, in <module>\n    a = session.run(model.forward_pass(model.X_batch, model.y_batch))\n  File \"/content/gdrive/My Drive/NLU/Projects/project 1/rok/models.py\", line 106, in forward_pass\n    lstm_output, (state_c, state_h) = self.LSTM(inputs=x_t, state=(state_c, state_h))  # 64x512\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 371, in __call__\n    *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__\n    self._maybe_build(inputs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build\n    self.build(input_shapes)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\", line 151, in wrapper\n    output_shape = fn(instance, input_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 720, in build\n    initializer=init_ops.zeros_initializer(dtype=self.dtype))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1232, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 435, in add_weight\n    getter=vs.get_variable)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 349, in add_weight\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 607, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2495, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1395, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1557, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 81, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3890, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value basic_lstm_cell/bias\n\t [[node basic_lstm_cell/bias/read (defined at /content/gdrive/My Drive/NLU/Projects/project 1/rok/models.py:106) ]]\n"]}]},{"metadata":{"id":"JwVUBSBSXDJQ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}