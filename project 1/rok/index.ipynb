{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_version:\t1.10.0\n"
     ]
    }
   ],
   "source": [
    "from load_embedding import load_embedding\n",
    "import utils\n",
    "import tf_utils\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"tf_version:\\t\" + tf.__version__)\n",
    "\n",
    "logger = utils.Logger(\"./logs/\")\n",
    "timer = utils.Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:                             20000          \n",
      "X_train:                                (1969833, 30)  \n",
      "X_test:                                 (10000, 30)    \n",
      "<bos> idx                               178            \n",
      "<eos> idx                               179            \n",
      "<pad> idx                               180            \n",
      "<unk> idx                               181            \n",
      "DATA LOADED.                            \n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# DIRECTORIES\n",
    "DATA_DIR = \"./data/\"\n",
    "RESULTS_DIR = \"./results/\"\n",
    "WORD_EMBEDDINGS_FILE = \"wordembeddings-dim100.word2vec\"\n",
    "SENTENCES_TRAIN_FILE = \"sentences.train\"\n",
    "SENTENCES_TEST_FILE = \"sentences_test.txt\"\n",
    "SENTENCES_EVAL_FILE = \"sentences.eval\"\n",
    "SENTENCES_CONTINUATION_FILE = \"sentences.continuation\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# LANGUAGE MODEL PARAMETERS\n",
    "EMBEDDING_DIM = 100\n",
    "STATE_DIM = 512\n",
    "VOCABULARY_SIZE = 20000\n",
    "SENT_DIM = 30\n",
    "\n",
    "# RNN PARAMETERS\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# LOAD DATA\n",
    "LOAD_DATA = True\n",
    "LOAD_EMBEDDING = False\n",
    "\n",
    "if LOAD_DATA:\n",
    "    with open(RESULTS_DIR + \"vocabulary.pkl\", \"rb\") as f:\n",
    "        vocabulary, word_to_idx, idx_to_word = pickle.load(f)\n",
    "        \n",
    "    X_train = np.load(RESULTS_DIR + \"X_train.npy\")\n",
    "    X_test = np.load(RESULTS_DIR + \"X_test.npy\")\n",
    "\n",
    "else:\n",
    "    vocabulary, word_to_idx, idx_to_word = utils.create_vocabulary(DATA_DIR + SENTENCES_TRAIN_FILE, VOCABULARY_SIZE)\n",
    "    X_train = utils.create_dataset(DATA_DIR + SENTENCES_TRAIN_FILE, word_to_idx)\n",
    "    X_test = utils.create_dataset(DATA_DIR + SENTENCES_TEST_FILE, word_to_idx)\n",
    "    \n",
    "    with open(RESULTS_DIR + \"vocabulary.pkl\", \"wb\") as f:\n",
    "        pickle.dump((vocabulary, word_to_idx, idx_to_word), f)\n",
    "    \n",
    "    with open(RESULTS_DIR + \"X_train.ids\", \"w\") as f:\n",
    "        for i in range(X_train.shape[0]):\n",
    "            f.write(\" \".join([str(x) for x in X_train[i, :]]) + \"\\n\")\n",
    "     \n",
    "    with open(RESULTS_DIR + \"X_test.ids\", \"w\") as f:\n",
    "        for i in range(X_test.shape[0]):\n",
    "            f.write(\" \".join([str(x) for x in X_test[i, :]]) + \"\\n\")\n",
    "    \n",
    "    np.save(RESULTS_DIR + \"X_train.npy\", X_train)\n",
    "    np.save(RESULTS_DIR + \"X_test.npy\", X_test)\n",
    "\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]    \n",
    "    \n",
    "logger.append(\"vocabulary:\", len(vocabulary))\n",
    "logger.append(\"X_train:\", X_train.shape)\n",
    "logger.append(\"X_test:\", X_test.shape)\n",
    "logger.append(\"<bos> idx\", word_to_idx[\"<bos>\"])\n",
    "logger.append(\"<eos> idx\", word_to_idx[\"<eos>\"])\n",
    "logger.append(\"<pad> idx\", word_to_idx[\"<pad>\"])\n",
    "logger.append(\"<unk> idx\", word_to_idx[\"<unk>\"])\n",
    "logger.append(\"DATA LOADED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1         batch: 1/157       loss = 633.8519287109375perplexity = 20008.978515625\n",
      "epoch: 1/1         batch: 6/157       loss = 554.6332397460938perplexity = 5989.4970703125\n",
      "epoch: 1/1         batch: 11/157       loss = 295.4862365722656perplexity = 285.9700927734375\n",
      "epoch: 1/1         batch: 16/157       loss = 312.59356689453125perplexity = 943.1192626953125\n",
      "epoch: 1/1         batch: 21/157       loss = 282.461669921875perplexity = 634.7378540039062\n",
      "epoch: 1/1         batch: 26/157       loss = 267.4273376464844perplexity = 294.16619873046875\n",
      "epoch: 1/1         batch: 31/157       loss = 284.5143127441406perplexity = 591.5167236328125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-487d2807e451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_perplexity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_count\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "np.random.seed(12345)\n",
    "tf.reset_default_graph()\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Dataset\n",
    "sentences_file_name = tf.placeholder(tf.string)\n",
    "\n",
    "training_dataset = tf.data.TextLineDataset(sentences_file_name).map(tf_utils.parse_ids_file).repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)\n",
    "X_batch, y_batch = iterator.get_next()\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "\n",
    "\n",
    "# Weights\n",
    "output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n",
    "                                initializer=initializer, trainable=True) # 512x20000\n",
    "if not LOAD_EMBEDDING:\n",
    "    embedding_weight = tf.get_variable(\"embedding_weight\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], \n",
    "                                       initializer=initializer, trainable=True) # 20000x100\n",
    "else:\n",
    "    embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[])  # 20000x100\n",
    "    \n",
    "# LSTM initialization\n",
    "batch_size = tf.shape(X_batch)[0] # Adjust for last batch\n",
    "LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n",
    "state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32) # 64x512\n",
    "\n",
    "X_batch_embedded = tf.nn.embedding_lookup(embedding_weight, X_batch)  # 64x29x100\n",
    "\n",
    "# logger.append(\"X_batch:\", X_batch.get_shape())  # 64x29\n",
    "# logger.append(\"y_batch:\", y_batch.get_shape())  # 64x29\n",
    "\n",
    "# logger.append(\"output_weight:\", output_weight.get_shape())  # 512x20000\n",
    "# logger.append(\"embedding_weight:\", embedding_weight.get_shape())  # 20000x100\n",
    "# logger.append(\"state_c:\", state_c.get_shape())  # 64x512\n",
    "# logger.append(\"state_h:\", state_h.get_shape())  # 64x512\n",
    "# logger.append(\"X_batch_embedded:\", X_batch_embedded.get_shape(), X_batch_embedded)  # 64x29x100\n",
    "\n",
    "losses = []\n",
    "probabilities = []\n",
    "\n",
    "# RNN forward pass\n",
    "for t in range(0, SENT_DIM - 1):\n",
    "    X_t = X_batch_embedded[:, t, :]  # 64x100\n",
    "    y_t = y_batch[:, t]  # 64x1\n",
    "    \n",
    "    lstm_output, (state_c, state_h) = LSTM(inputs=X_t, state=(state_c, state_h))  # 64x512\n",
    "    logits = tf.matmul(lstm_output, output_weight)  # 64x20000\n",
    "    \n",
    "    loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)  # 64x1\n",
    "    losses.append(loss_t)\n",
    "    \n",
    "    probability_t = tf.math.exp(-loss_t)\n",
    "    probabilities.append(probability_t)\n",
    "    \n",
    "    # Same as sparse softmax\n",
    "    # probabilities_t = tf.nn.softmax(logits=logits, axis=1) # 64x20000\n",
    "    # probabilities_t = tf.reduce_sum(tf.multiply(probabilities_t, tf.one_hot(y_t, depth=VOCABULARY_SIZE,dtype=tf.float32)), axis=1)\n",
    "    \n",
    "    \n",
    "losses = tf.stack(losses)  # 29x64 \n",
    "loss = tf.reduce_mean(tf.reduce_sum(losses,axis=1))  # 29x1 -> 1x1\n",
    "\n",
    "perplexity = tf.reduce_mean(tf.exp(tf.reduce_mean(losses, axis=0))) # exp(-1/n sum_t=1...n  -log p(w_t|w_1:t-1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n",
    "optimize_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # Initialize variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load data\n",
    "    session.run(training_init_op, {sentences_file_name: RESULTS_DIR + \"X_test.ids\"})\n",
    "    \n",
    "    # Load embedding\n",
    "    if LOAD_EMBEDDING:\n",
    "        load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n",
    "\n",
    "    epoch = 0\n",
    "    batch_count = 0\n",
    "    total_batch = num_test / BATCH_SIZE\n",
    "    while True:\n",
    "\n",
    "        try:\n",
    "            batch_loss, _, batch_perplexity = session.run([loss, optimize_op, perplexity])\n",
    "            epoch = floor(batch_count / total_batch) + 1\n",
    "            \n",
    "            if batch_count % 5 == 0:\n",
    "                print(\"epoch: {}/{:<10}batch: {}/{:<10}loss = {:<5}perplexity = {}\".format(epoch, NUM_EPOCHS, \n",
    "                                                                         batch_count + 1, ceil(total_batch), batch_loss, batch_perplexity))\n",
    "            \n",
    "            batch_count += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# PARAMETERS\n",
    "len_sents = X_train.shape[1]\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "# Initializer\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Parameters\n",
    "output_weight = tf.get_variable(\"output_weight\", shape=[STATE_DIM, VOCABULARY_SIZE], \n",
    "                                initializer=initializer, trainable=True)\n",
    "\n",
    "if not load_embedding:\n",
    "    embedding_weight = tf.get_variable(\"embedding_weight\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], \n",
    "                                       initializer=initializer, trainable=True)\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.int32, (None, sent_dim))\n",
    "\n",
    "# LSTM initialization\n",
    "LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n",
    "state_c, state_h = LSTM.zero_state(batch_size=BATCH_SIZE, dtype=tf.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# RNN forward pass\n",
    "for t in range(0, sent_dim - 1):\n",
    "    X_t = X[:, t]\n",
    "    y_t = X[:, t+1] # 64x1\n",
    "    \n",
    "    X_t = tf.one_hot(X_t, depth = VOCABULARY_SIZE)\n",
    "    E_t = tf.matmul(X_t, E)\n",
    "\n",
    "    output, (state_c, state_h) = LSTM(inputs=E_t, state=(state_c, state_h))\n",
    "    logits = tf.matmul(output, W)\n",
    "    \n",
    "    loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)\n",
    "    \n",
    "    losses.append(loss_t)\n",
    "    \n",
    "losses = tf.reduce_sum(tf.stack(losses),axis=1)\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n",
    "optimize_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------#\n",
    "# SESSION\n",
    "np.random.seed(12345)\n",
    "\n",
    "batches_per_epoch = 20\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    session.run(init)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "#         print('epoch\\t%4d' % epoch + 1)\n",
    "        \n",
    "        for idx in range(batches_per_epoch):\n",
    "            batch_loss, _ = session.run([loss, optimize_op],\n",
    "                                    feed_dict={X: X_test[(idx*BATCH_SIZE):((idx+1)*BATCH_SIZE)]}\n",
    "                                    )\n",
    "            epoch_loss += batch_loss\n",
    "            print('\\tbatch %4d\\t%.2f' % (idx + 1, batch_loss))\n",
    "        \n",
    "#         if epoch + 1 % 2 == 0:\n",
    "        print('epoch\\t%4d\\t%.2f' % (epoch + 1, epoch_loss / batches_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_GRAD_NORM = 5.0\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "STATE_DIM = 512\n",
    "VOCABULARY_SIZE = 20000\n",
    "\n",
    "sent_dim = X_train.shape[1]\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "batch_per_epoch = floor(num_test / BATCH_SIZE)\n",
    "\n",
    "# Session\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initializer\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# Parameters\n",
    "W = tf.get_variable(\"W\", shape=[STATE_DIM, VOCABULARY_SIZE], initializer=initializer, trainable=True)\n",
    "E = tf.get_variable(\"E\", shape=[VOCABULARY_SIZE, EMBEDDING_DIM], initializer=initializer, trainable=True)\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(tf.int32, (None, sent_dim))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X).batch(batch_size).repeat()\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "X_batch = iterator.get_next()\n",
    "\n",
    "# LSTM initialization\n",
    "LSTM = tf.nn.rnn_cell.BasicLSTMCell(num_units=STATE_DIM)\n",
    "state_c, state_h = LSTM.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# RNN forward pass\n",
    "for t in range(0, 5):\n",
    "    X_t = X_batch[:, t]\n",
    "    y_t = X_batch[:, t+1]\n",
    "    \n",
    "    X_t = tf.one_hot(X_t, depth = VOCABULARY_SIZE)\n",
    "    E_t = tf.matmul(X_t, E)\n",
    "\n",
    "    output, (state_c, state_h) = LSTM(inputs=E_t, state=(state_c, state_h))\n",
    "    logits = tf.matmul(output, W)\n",
    "    \n",
    "    loss_t = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=logits)\n",
    "    \n",
    "    losses.append(loss_t)\n",
    "    \n",
    "losses = tf.stack(losses)\n",
    "losses = tf.reduce_sum(losses,axis=1)\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, clip_norm=MAX_GRAD_NORM)\n",
    "optimize_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "# SESSION\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(init)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for _ in range(batch_per_epoch):\n",
    "        batch_loss, _ = sess.run([loss, train_op, loss])\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        \n",
    "#     train_loss, _ = session.run([loss, optimize_op],\n",
    "#                             feed_dict={X: X_test[0:batch_size]}\n",
    "#                             )\n",
    "\n",
    "    if epoch + 1 % 1 == 0:\n",
    "        print('Epoch %04d> training loss: %.2f' % (epoch, total_loss))\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external embeddings from ./data/wordembeddings-dim100.word2vec\n",
      "<bos> not in embedding file\n",
      "<eos> not in embedding file\n",
      "<pad> not in embedding file\n",
      "<unk> not in embedding file\n",
      "19996 words out of 20000 could be loaded\n",
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_weight = tf.Variable(np.empty((VOCABULARY_SIZE, EMBEDDING_DIM), dtype=np.float32), collections=[])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    load_embedding(session, word_to_idx, embedding_weight, DATA_DIR + WORD_EMBEDDINGS_FILE, EMBEDDING_DIM, VOCABULARY_SIZE)\n",
    "    print(embedding_weight.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
