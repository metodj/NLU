{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"index.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DYOLuNB7BQf8","colab_type":"code","outputId":"24768a35-bfcc-4cfa-8c78-19a57112bb5e","executionInfo":{"status":"ok","timestamp":1556360780041,"user_tz":-120,"elapsed":73698,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"cell_type":"code","source":["import os\n","import platform\n","import sys\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","os.chdir(\"./gdrive/My Drive/NLU/Projects/project 1/rok/\")\n","\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","from model import Model\n","from load_embedding import load_embedding\n","import utils\n","import tf_utils\n","\n","logger = utils.Logger(\"./logs/\")\n","timer = utils.Timer()\n","\n","# !pip install tensorboardcolab\n","# from tensorboardcolab import *\n","# tbc = TensorBoardColab()\n","\n","logger.append(\"SYSTEM\", platform.system())\n","logger.append(\"MACHINE\", platform.machine())\n","logger.append(\"PLATFORM\", platform.platform())\n","logger.append(\"UNAME\", platform.uname(), \"\\n\")\n","\n","logger.append(\"PYTHON\", sys.version.split('\\n'))\n","logger.append(\"TF VERSION\", tf.__version__, \"\\n\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n","SYSTEM                                  Linux          \n","MACHINE                                 x86_64         \n","PLATFORM                                Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n","UNAME                                   uname_result(system='Linux', node='5b9640280c94', release='4.14.79+', version='#1 SMP Wed Dec 19 21:19:13 PST 2018', machine='x86_64', processor='x86_64')\n","              \n","PYTHON                                  ['3.6.7 (default, Oct 22 2018, 11:32:17) ', '[GCC 8.2.0]']\n","TF VERSION                              1.13.1         \n","              \n"],"name":"stdout"}]},{"metadata":{"id":"m43Oi1wbAzyi","colab_type":"code","outputId":"1dba1dad-1945-427e-a15c-b362a93095f2","executionInfo":{"status":"ok","timestamp":1556360927132,"user_tz":-120,"elapsed":125842,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":1071}},"cell_type":"code","source":["# -------------------------------------------------------------------------------------------------------------------- #\n","# MODEL\n","tf.app.flags.DEFINE_string(\"EXPERIMENT\", \"C\", \"model selection (A, B, C)\")\n","tf.app.flags.DEFINE_string(\"MODE\", \"G\", \"mode (Experiment - E, Generation - G)\")\n","tf.app.flags.DEFINE_boolean(\"RESTORE\", True, \"Restore existing model\")\n","\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# DIRECTORIES AND FILES\n","tf.app.flags.DEFINE_string(\"DATA_DIR\", \"./data/\", \"data directory\")\n","tf.app.flags.DEFINE_string(\"RESULTS_DIR\", \"./results/\", \"results directory\")\n","tf.app.flags.DEFINE_string(\"MODEL_DIR\", \"./model/\", \"saved model directory\")\n","tf.app.flags.DEFINE_string(\"WORD_EMBEDDINGS_FILE\", \"wordembeddings-dim100.word2vec\", \"word embedding file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_TRAIN_FILE\", \"sentences.train\", \"train file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_TEST_FILE\", \"sentences_test.txt\", \"test file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_EVAL_FILE\", \"sentences.eval\", \"evaluation file\")\n","tf.app.flags.DEFINE_string(\"SENTENCES_CONTINUATION_FILE\", \"sentences.continuation\", \"continuation file\")\n","\n","# LANGUAGE MODEL PARAMETERS\n","tf.app.flags.DEFINE_integer(\"EMBEDDING_DIM\", 100, \"word embedding dimension\")\n","tf.app.flags.DEFINE_integer(\"DOWN_STATE_DIM\", 512, \"down projection dimension\")\n","tf.app.flags.DEFINE_integer(\"STATE_DIM\", 512, \"rnn cell hidden state dimension\")\n","tf.app.flags.DEFINE_integer(\"VOCABULARY_SIZE\", 20000, \"vocabulary size\")\n","tf.app.flags.DEFINE_integer(\"SENT_DIM\", 30, \"train sentence length\")\n","tf.app.flags.DEFINE_integer(\"CONT_DIM\", 20, \"continuation max. sentence length\")\n","\n","# RNN PARAMETERS\n","tf.app.flags.DEFINE_integer(\"BATCH_SIZE\", 64, \"batch size\")\n","tf.app.flags.DEFINE_integer(\"NUM_EPOCHS\", 1, \"number of epochs for training\")\n","tf.app.flags.DEFINE_float(\"LEARNING_RATE\", 0.001, \"learning rate for rnn\")\n","tf.app.flags.DEFINE_float(\"MAX_GRAD_NORM\", 5.0, \"max. norm for gradient clipping\")\n","tf.app.flags.DEFINE_string('f', '', 'tensorflow bug')\n","\n","FLAGS = tf.app.flags.FLAGS\n","if FLAGS.EXPERIMENT == \"C\":\n","    FLAGS.STATE_DIM = 1024\n","tf_utils.print_flags(FLAGS, logger)\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# PREPROCESSING\n","logger.append(\"PREPROCESSING STARTING.\")\n","vocabulary, word_to_idx, idx_to_word = utils.create_vocabulary(FLAGS.DATA_DIR + FLAGS.SENTENCES_TRAIN_FILE, FLAGS.VOCABULARY_SIZE)\n","X_train = utils.create_dataset(FLAGS.DATA_DIR + FLAGS.SENTENCES_TRAIN_FILE, word_to_idx)\n","logger.append(\"X_train CREATED.\")\n","X_test = utils.create_dataset(FLAGS.DATA_DIR + FLAGS.SENTENCES_TEST_FILE, word_to_idx)\n","logger.append(\"X_test CREATED.\")\n","X_eval = utils.create_dataset(FLAGS.DATA_DIR + FLAGS.SENTENCES_EVAL_FILE, word_to_idx)\n","logger.append(\"X_eval CREATED.\")\n","X_cont = utils.load_continuation(FLAGS.DATA_DIR + FLAGS.SENTENCES_CONTINUATION_FILE, word_to_idx)\n","logger.append(\"X_cont CREATED.\")\n","\n","with open(FLAGS.RESULTS_DIR + \"vocabulary.pkl\", \"wb\") as f:\n","    pickle.dump((vocabulary, word_to_idx, idx_to_word), f)\n","\n","with open(FLAGS.RESULTS_DIR + \"X_train.ids\", \"w\") as f:\n","    for i in range(X_train.shape[0]):\n","        f.write(\" \".join([str(x) for x in X_train[i, :]]) + \"\\n\")\n","\n","with open(FLAGS.RESULTS_DIR + \"X_test.ids\", \"w\") as f:\n","    for i in range(X_test.shape[0]):\n","        f.write(\" \".join([str(x) for x in X_test[i, :]]) + \"\\n\")\n","\n","with open(FLAGS.RESULTS_DIR + \"X_eval.ids\", \"w\") as f:\n","    for i in range(X_eval.shape[0]):\n","        f.write(\" \".join([str(x) for x in X_eval[i, :]]) + \"\\n\")\n","\n","with open(FLAGS.RESULTS_DIR + \"X_cont.ids\", \"w\") as f:\n","    for i in range(X_cont.shape[0]):\n","        f.write(\" \".join([str(x) for x in X_cont[i, :]]) + \"\\n\")\n","        \n","num_train = X_train.shape[0]\n","num_test = X_test.shape[0]\n","num_eval = X_eval.shape[0]\n","num_cont = X_cont.shape[0]\n","    \n","logger.append(\"vocabulary:\", len(vocabulary))\n","logger.append(\"X_train:\", X_train.shape)\n","logger.append(\"X_test:\", X_test.shape)\n","logger.append(\"X_eval:\", X_eval.shape)\n","logger.append(\"<bos> idx\", word_to_idx[\"<bos>\"])\n","logger.append(\"<eos> idx\", word_to_idx[\"<eos>\"])\n","logger.append(\"<pad> idx\", word_to_idx[\"<pad>\"])\n","logger.append(\"<unk> idx\", word_to_idx[\"<unk>\"])\n","logger.append(\"PREPROCESSING FINISHED.\\n\")\n","\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# LOAD VOCABULARY\n","with open(FLAGS.RESULTS_DIR + \"vocabulary.pkl\", \"rb\") as f:\n","    vocabulary, word_to_idx, idx_to_word = pickle.load(f)\n","\n","logger.append(\"VOCABULARY LOADED.\\n\")\n","# -------------------------------------------------------------------------------------------------------------------- #\n","# RUN\n","tf.reset_default_graph()\n","tf.set_random_seed(12345)\n","np.random.seed(12345)\n","\n","model = Model(experiment=FLAGS.EXPERIMENT,\n","              mode=FLAGS.MODE,\n","              vocabulary_size=FLAGS.VOCABULARY_SIZE,\n","              embedding_dim=FLAGS.EMBEDDING_DIM,\n","              state_dim=FLAGS.STATE_DIM,\n","              down_state_dim=FLAGS.DOWN_STATE_DIM,\n","              sent_dim=FLAGS.SENT_DIM,\n","              cont_dim=FLAGS.CONT_DIM,\n","              initializer=tf.contrib.layers.xavier_initializer(),\n","              pad_idx=word_to_idx[\"<pad>\"],\n","              eos_idx=word_to_idx[\"<eos>\"],\n","              num_epochs=FLAGS.NUM_EPOCHS\n","              )\n","logger.append(\"TRAINABLE VARIABLES.\")\n","tf_utils.trainable_parameters(logger)\n","\n","saver = tf.train.Saver()\n","timer.__enter__()\n","\n","logger.append(\"TF SESSION STARTING.\\n\")\n","with tf.Session() as session:\n","#     writer = tbc.get_deep_writers(\"./\")\n","#     writer.add_graph(session.graph)\n","    \n","    if FLAGS.MODE == \"E\":\n","        logger.append(\"EXPERIMENT STARTING.\")\n","        with tf.name_scope(\"experiment\"):\n","            if not FLAGS.RESTORE:\n","                session.run(tf.global_variables_initializer())\n","\n","                # LOAD EMBEDDING\n","                if FLAGS.EXPERIMENT == \"B\":\n","                    load_embedding(session, word_to_idx, model.embedding_weight,\n","                                   FLAGS.DATA_DIR + FLAGS.WORD_EMBEDDINGS_FILE, \n","                                   FLAGS.EMBEDDING_DIM, FLAGS.VOCABULARY_SIZE)\n","            else:\n","                saver.restore(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                              FLAGS.EXPERIMENT + \"/experiment\" + \n","                              FLAGS.EXPERIMENT + \".ckpt\")\n","                logger.append(\"MDOEL RESTORED.\")\n","       \n","            # TRAINING\n","#             summary_op = tf.summary.merge_all()\n","          \n","            session.run(model.iterator_op,\n","                        {model.sentences_file: FLAGS.RESULTS_DIR + \"X_train.ids\"})\n","\n","            logger.append(\"TRAINING STARTING.\")\n","            batch_count = 0\n","            batch_perplexity = 100\n","            total_batch = num_train // FLAGS.BATCH_SIZE + 1\n","            while True:\n","                try:\n","#                     batch_loss, batch_perplexity, _, global_step, summary = session.run([model.loss, model.perplexity, \n","#                                                                    model.optimize_op, model.global_step, summary_op])\n","                  \n","                    batch_loss, _, global_step = session.run([model.loss, \n","                                               model.optimize_op, model.global_step])\n","                    \n","                    \n","#                     writer.add_summary(summary, global_step)\n","                    epoch = 1\n","                    if batch_count % 100 == 0:\n","                        logger.append(\"batch: {:>5}/{:<6}\".format(batch_count + 1, total_batch), \"loss = {:<8.2f}\".format(batch_loss), \"perp = {:<8.2f}\".format(batch_perplexity))\n","                        \n","                    batch_count += 1\n","                except tf.errors.OutOfRangeError:\n","                    break\n","            \n","            logger.append(\"TRAINING FINISHED.\")\n","#             writer.flush()\n","            save_path = saver.save(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                                   FLAGS.EXPERIMENT + \"/experiment\" + \n","                                   FLAGS.EXPERIMENT + \".ckpt\")\n","            logger.append(\"MODEL SAVED\", save_path)\n","\n","            # EVALUATION\n","            logger.append(\"EVALUATION STARTING.\")\n","            session.run(model.iterator_op, {model.sentences_file: FLAGS.RESULTS_DIR + \"X_test.ids\"})\n","            eval_perplexities = np.array([], dtype=np.float32)\n","            batch_count = 0\n","            while True:\n","                try:\n","                    batch_perplexities = session.run(model.perplexities)\n","                    eval_perplexities = np.append(eval_perplexities, batch_perplexities)\n","                    batch_count += 1\n","                except tf.errors.OutOfRangeError:\n","                    break\n","            logger.append(\"EVALUATION FINISHED.\")\n","\n","            with open(FLAGS.RESULTS_DIR + \"group23.perplexity\" + FLAGS.EXPERIMENT, \"w\") as f:\n","                for i in range(eval_perplexities.shape[0]):\n","                    f.write(\"%0.3f\" % eval_perplexities[i] + \"\\n\")\n","\n","            logger.append(\"EXPERIMENT FINISHED.\\n\")\n","    elif FLAGS.MODE == \"G\":\n","        logger.append(\"GENERATION STARTING.\")\n","        with tf.name_scope(\"generation\"):\n","            saver.restore(session, FLAGS.MODEL_DIR + \"/experiment\" + \n","                          FLAGS.EXPERIMENT + \"/experiment\" + \n","                          FLAGS.EXPERIMENT + \".ckpt\")\n","            logger.append(\"MODEL RESTORED.\")\n","            \n","            session.run(model.iterator_op, {model.sentences_file: FLAGS.RESULTS_DIR + \"X_cont.ids\"})\n","\n","\n","            continuation_ids = []\n","            batch_count = 0\n","            while True:\n","                try:\n","                    batch_predictions = session.run(model.predictions)\n","                    continuation_ids.append(batch_predictions)\n","                    batch_count = batch_count + 1\n","\n","                    print(batch_count, end=\"\\r\")\n","                except tf.errors.OutOfRangeError:\n","                    break\n","\n","            continuation_ids = np.concatenate(continuation_ids, axis=0)\n","            print(continuation_ids.shape)\n","\n","            with open(FLAGS.RESULTS_DIR + \"group23.continuation\", \"w\") as f:\n","                for i in range(continuation_ids.shape[0]):\n","                    try:\n","                        eos_pos = continuation_ids[i, 1:].tolist().index(int(word_to_idx[\"<eos>\"]))\n","                    except:\n","                        eos_pos = 20\n","\n","                    gen_sent = \" \".join([idx_to_word[token_id] if idx <= eos_pos else \"\" for idx, token_id in\n","                                         enumerate(continuation_ids[i, 1:].tolist())])\n","                    f.write(gen_sent + \"\\n\")\n","        logger.append(\"GENERATION FINISHED.\\n\")\n","    logger.append(\"SESSION FINISHING.\\n\")\n","timer.__exit__()\n","tf_utils.delete_flags(FLAGS)\n","logger.create_log()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["EXPERIMENT                              C              \n","MODE                                    G              \n","RESTORE                                 True           \n","DATA_DIR                                ./data/        \n","RESULTS_DIR                             ./results/     \n","MODEL_DIR                               ./model/       \n","WORD_EMBEDDINGS_FILE                    wordembeddings-dim100.word2vec\n","SENTENCES_TRAIN_FILE                    sentences.train\n","SENTENCES_TEST_FILE                     sentences_test.txt\n","SENTENCES_EVAL_FILE                     sentences.eval \n","SENTENCES_CONTINUATION_FILE             sentences.continuation\n","EMBEDDING_DIM                           100            \n","DOWN_STATE_DIM                          512            \n","STATE_DIM                               1024           \n","VOCABULARY_SIZE                         20000          \n","SENT_DIM                                30             \n","CONT_DIM                                20             \n","BATCH_SIZE                              64             \n","NUM_EPOCHS                              1              \n","LEARNING_RATE                           0.001          \n","MAX_GRAD_NORM                           5.0            \n","PREPROCESSING STARTING.                 \n","X_train CREATED.                        \n","X_test CREATED.                         \n","X_eval CREATED.                         \n","X_cont CREATED.                         \n","vocabulary:                             20000          \n","X_train:                                (1969833, 30)  \n","X_test:                                 (10000, 30)    \n","X_eval:                                 (9846, 30)     \n","<bos> idx                               178            \n","<eos> idx                               179            \n","<pad> idx                               180            \n","<unk> idx                               181            \n","PREPROCESSING FINISHED.\n","                \n","VOCABULARY LOADED.\n","                     \n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","TRAINABLE VARIABLES.                    \n","output_weight:0                         (512, 20000)   \n","embedding_weight:0                      (20000, 100)   \n","down_weight:0                           (1024, 512)    \n","lstm_cell/kernel:0                      (1124, 4096)   \n","lstm_cell/bias:0                        (4096,)        \n","num_parameters                          17372288       \n","TF SESSION STARTING.\n","                   \n","GENERATION STARTING.                    \n","MODEL RESTORED.                         \n","(10000, 21)\n","GENERATION FINISHED.\n","                   \n","SESSION FINISHING.\n","                     \n","Elapsed: 14.356679201126099s\n"],"name":"stdout"}]},{"metadata":{"id":"nvF-hVUs2-6q","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}