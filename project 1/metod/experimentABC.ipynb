{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "os.chdir(\"./gdrive/My Drive/NLU/project1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tensorboardcolab import *\n",
    "# tbc = TensorBoardColab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id_dict = json.load(open(\"dictionary_0index.txt\"))\n",
    "inverse_vocab = json.load(open(\"dictionary_inverse.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TRAIN = 'sentences.train'\n",
    "FILE_EVAL = 'sentences.eval'\n",
    "FILE_TEST = 'sentences_test.txt'\n",
    "FILE_SENTENCES = 'sentences.continuation'\n",
    "\n",
    "INPUT_TRAIN = 'train.ids'\n",
    "INPUT_EVAL = 'eval.ids'\n",
    "INPUT_TEST = 'test.ids'\n",
    "INPUT_SENTENCES = 'sentences.ids'\n",
    "EMBEDDING_FILE = 'wordembeddings-dim100.word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'A'\n",
    "BATCH_SIZE=64\n",
    "VOCAB_SIZE = 20000\n",
    "HIDDEN_UNITS = 512\n",
    "EMBED_SIZE = 100\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT = 'B'\n",
    "BATCH_SIZE=64\n",
    "VOCAB_SIZE = 20000\n",
    "HIDDEN_UNITS = 512\n",
    "EMBED_SIZE = 100\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT = 'C'\n",
    "BATCH_SIZE=64\n",
    "VOCAB_SIZE = 20000\n",
    "HIDDEN_UNITS = 1024\n",
    "EMBED_SIZE = 100\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODE = 'train'\n",
    "#MODE = 'perplexity_test'\n",
    "MODE = 'generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = model.Model(batch_size=BATCH_SIZE, vocab_size=VOCAB_SIZE, hidden_units=HIDDEN_UNITS, \n",
    "              embed_size=EMBED_SIZE, num_epochs=NUM_EPOCHS, experiment=EXPERIMENT)\n",
    "\n",
    "print(\"Number of trainable parameters: {}\".format(count_trainable_parameters()))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if MODE == 'train':\n",
    "        writer = tbc.get_deep_writers(\"./\")\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if model.experiment == \"B\":\n",
    "            load_embedding(sess, word_to_id_dict, model.input_embedding_mat, EMBEDDING_FILE, model.embed_size, model.vocab_size)\n",
    "        epoch = 0\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        while epoch < model.num_epochs:\n",
    "\n",
    "            #training step\n",
    "            sess.run(model.training_init_op, {model.file_name_train: INPUT_TRAIN})\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    _loss, _global_step, _ , summary = sess.run([model.loss, model.global_step, model.updates, summary_op])\n",
    "\n",
    "                    writer.add_summary(summary, _global_step)\n",
    "\n",
    "                    if _global_step % 200 == 0:\n",
    "                        duration = time.time()-start \n",
    "                        print(\"Batch: {}. Epoch: {} Loss: {} Time: {} seconds. \".format(_global_step, epoch, np.sum(_loss), duration))\n",
    "                        start = time.time()\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    # The end of one epoch\n",
    "                    epoch += 1\n",
    "                    break \n",
    "\n",
    "\n",
    "            #validation step\n",
    "            sess.run(model.validation_init_op, {model.file_name_validation: INPUT_EVAL})\n",
    "            eval_loss = []\n",
    "            eval_ppl = []\n",
    "            while True:\n",
    "                try:\n",
    "                    _eval_loss, _eval_ppl = sess.run([model.loss_, model.perplexity_])\n",
    "                    eval_loss.append(np.sum(_eval_loss))\n",
    "                    eval_ppl.append(_eval_ppl)\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print(\"Epoch: {} Avg eval loss per batch: {}. Avg eval ppl per batch: {} \".format(epoch, \\\n",
    "                                                                                      np.mean(eval_loss), np.mean(eval_ppl)))\n",
    "                    break\n",
    "\n",
    "\n",
    "        #after training is done, save the model\n",
    "        save_path = saver.save(sess, \"model\" + EXPERIMENT + \".ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "        writer.flush()\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        saver.restore(sess, \"model\" + EXPERIMENT + \".ckpt\")\n",
    "        if model.experiment == \"B\":\n",
    "            load_embedding(sess, word_to_id_dict, model.input_embedding_mat, EMBEDDING_FILE, model.embed_size, model.vocab_size)\n",
    "        \n",
    "        if MODE == 'perplexity_test':\n",
    "            sess.run(model.test_init_op, {model.file_name_test: INPUT_TEST})\n",
    "            with open ('results' + EXPERIMENT + '.txt', 'w') as file:\n",
    "                while True:\n",
    "                    try:\n",
    "                        test_ppl = sess.run(model.perplexity_)\n",
    "                        file.write(str(test_ppl) + '\\n')\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                    \n",
    "        elif MODE == 'generation':\n",
    "            sess.run(model.test_init_op, {model.file_name_test: INPUT_SENTENCES})\n",
    "            with open ('sentence_generation' + EXPERIMENT + '.txt', 'w') as file:\n",
    "            while True:\n",
    "                try:\n",
    "                    sentence = sess.run(model.preds)\n",
    "                    sentence = [inverse_vocab[str(i)] for i in sentence][1:] #skip the <bos> tag in the output file\n",
    "                    file.write(' '.join(sentence) + '\\n')\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4644a5f3f64c87a11cce7742d32be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('A', 'B', 'C'), value='A'), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def results(name=['A', 'B', 'C']):\n",
    "    results = pd.read_csv('results/results' + name + '.txt', header=None) \n",
    "    results.columns = ['perplexity']\n",
    "    print('Mean: ' + str(results.perplexity.mean()))\n",
    "    print('Standard deviation ' + str(results.perplexity.std()))\n",
    "    print('Median ' + str(results.perplexity.quantile(0.50)))\n",
    "    return results.perplexity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIFFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61ab9be9c1f4a0ca86ebbf1c79ea527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('A', 'B', 'C'), value='A'), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def results(name=['A', 'B', 'C']):\n",
    "    res_metod = pd.read_csv('results/results' + name + '.txt', header=None)\n",
    "    res_siki = pd.read_csv('../rok/results/group23.perplexity' + name, header=None)\n",
    "    tmp = pd.concat([res_metod, res_siki], axis=1)\n",
    "    tmp.columns = ['metod', 'siki']\n",
    "    tmp['diff'] = abs(tmp.metod-tmp.siki)\n",
    "    return tmp['diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
