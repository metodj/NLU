{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_sct _v2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yecJhSfvrIxr","colab_type":"code","outputId":"90563e18-7fe5-4b6a-8ef5-578e3fea5680","executionInfo":{"status":"ok","timestamp":1560255852256,"user_tz":-120,"elapsed":8753,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import sys\n","import os\n","import datetime\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","!pip install bert-tensorflow --quiet\n","from bert import tokenization\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","sys.path.append(\"./gdrive/My Drive/NLU/Projects/project_2/\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0611 12:24:06.224715 139882778761088 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BRPZ9_J7sz6X","colab_type":"code","colab":{}},"source":["flags = tf.flags\n","FLAGS = flags.FLAGS\n","\n","flags.DEFINE_string(\"data_dir\", \"./gdrive/My Drive/NLU/Projects/project_2/data_pp_test/\", \n","                    \"The input data dir. Should contain the .tsv files (or other data files).\")\n","flags.DEFINE_string(\"output_dir\", \"./output_G/\", \"The output directory where the model checkpoints will be written.\")\n","flags.DEFINE_string(\"results_dir\", \"./results_G/\", \"The output directory for result files.\")\n","\n","# Pretrained BERT from hub\n","flags.DEFINE_string(\"bert_model_hub\", \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", \"Pretrained BERT. (default)\")\n","flags.DEFINE_string(\"bert_dir\", None, \"Local directory of pretrained BERT.\")\n","flags.DEFINE_bool(\"bert_trainable\", False, \"Whether BERT weights are trainable.\")\n","flags.DEFINE_string(\"init_checkpoint\", None, \"Initial checkpoint of BERT.\")\n","flags.DEFINE_string(\"init_checkpoint_sent\", None, \"Initial checkpoint of pre-trained sentiment model.\")\n","# flags.DEFINE_string(\"init_checkpoint_sent\", \"./gdrive/My Drive/NLU/Projects/project_2/sentiment_pp/sentiment_0.ckpt\", \"Initial checkpoint of pre-trained sentiment model.\")\n","\n","# Pretrained Masked LM BERT\n","# flags.DEFINE_string(\"bert_model_hub\", None, \"Pretrained BERT. (default)\")\n","# flags.DEFINE_string(\"bert_dir\", \"./gdrive/My Drive/NLU/Projects/project_2/bert/\", \"Local directory of pretrained BERT.\")\n","# flags.DEFINE_bool(\"bert_trainable\", True, \"Whether BERT weights are trainable.\")\n","# flags.DEFINE_string(\"init_checkpoint\", \"./gdrive/My Drive/NLU/Projects/project_2/bert/bert_model.ckpt\", \"Initial checkpoint of BERT.\")\n","# flags.DEFINE_string(\"init_checkpoint\", \"./gdrive/My Drive/NLU/Projects/project_2/bert_masked_lm_pp/model.ckpt\", \"Initial checkpoint of BERT.\")\n","\n","# Training or evaluation\n","flags.DEFINE_bool(\"do_train\", True, \"Whether to run training.\")\n","flags.DEFINE_bool(\"do_eval\", True, \"Whether to run eval on the dev set.\")\n","flags.DEFINE_bool(\"do_predict\", True, \"Whether to run the model in inference mode on the test set.\")\n","\n","# Model selection\n","flags.DEFINE_bool(\"sentiment_only\", False, \"sentiment only\")\n","flags.DEFINE_bool(\"commonsense_only\", True, \"commonsense only\")\n","flags.DEFINE_bool(\"combination\", False, \"combination\")\n","\n","flags.DEFINE_integer(\"max_seq_length\", 400, \"Max. length after tokenization. If shorter padded, else truncated.\")\n","flags.DEFINE_integer(\"batch_size\", 8, \"Total batch size for training.\")\n","flags.DEFINE_float(\"learning_rate\", 2e-5, \"The initial learning rate for Adam.\")\n","flags.DEFINE_float(\"num_train_epochs\", 3.0, \"Total number of training epochs to perform.\")\n","flags.DEFINE_float(\"warmup_proportion\", 0.1, \"Proportion of training to perform linear learning rate warmup for.\")\n","tf.app.flags.DEFINE_string('f', '', 'tensorflow bug')\n","\n","if not os.path.exists(FLAGS.output_dir):\n","    os.makedirs(FLAGS.output_dir)\n","    \n","if not os.path.exists(FLAGS.results_dir):\n","    os.makedirs(FLAGS.results_dir)\n","    \n","timestamp = datetime.datetime.now().strftime(\"%d_%H-%M\")\n","\n","from bert_sct_utils_v2 import create_tokenizer_from_hub_module, SctProcessor, get_config\n","from bert_sct_utils_v2 import model_fn_builder, file_based_input_fn_builder, file_based_convert_examples_to_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXTiMQkFruBs","colab_type":"code","outputId":"5b273b6f-d7a7-416b-d381-9f27e6775a9b","executionInfo":{"status":"ok","timestamp":1560141840418,"user_tz":-120,"elapsed":331897,"user":{"displayName":"Rok Šikonja","photoUrl":"","userId":"08390144229917873056"}},"colab":{"base_uri":"https://localhost:8080/","height":1873}},"source":["if not(FLAGS.bert_model_hub or (FLAGS.bert_dir and FLAGS.init_checkpoint)):\n","    raise ValueError(\"bert_model_hub or (bert_config_file and init_checkpoint)\")\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Create output directory\n","tf.gfile.MakeDirs(FLAGS.output_dir)\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Processor\n","processor = SctProcessor()\n","label_list = processor.get_labels()\n","\n","bert_config = None\n","if not FLAGS.init_checkpoint:\n","    tokenizer = create_tokenizer_from_hub_module(FLAGS.bert_model_hub)\n","\n","else:\n","    tokenizer = tokenization.FullTokenizer(vocab_file=os.path.join(FLAGS.bert_dir, \"vocab.txt\"),\n","                                           do_lower_case=False)\n","    bert_config = get_config(os.path.join(FLAGS.bert_dir, \"bert_config.json\"))\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Model and Estimator\n","train_examples = None\n","num_train_steps = None\n","num_warmup_steps = None\n","if FLAGS.do_train:\n","    train_examples = processor.get_train_examples(FLAGS.data_dir)\n","    num_train_steps = int(len(train_examples) / FLAGS.batch_size * FLAGS.num_train_epochs)\n","    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n","\n","run_config = tf.estimator.RunConfig(model_dir=FLAGS.output_dir,\n","                                    save_summary_steps=10,\n","                                    save_checkpoints_steps=1000)\n","\n","model_fn = model_fn_builder(bert_model_hub=FLAGS.bert_model_hub,\n","                            bert_trainable=FLAGS.bert_trainable,\n","                            bert_config=bert_config,\n","                            init_checkpoint=FLAGS.init_checkpoint,\n","                            init_checkpoint_sent=FLAGS.init_checkpoint_sent,\n","                            num_labels=len(label_list),\n","                            learning_rate=FLAGS.learning_rate,\n","                            num_train_steps=num_train_steps,\n","                            num_warmup_steps=num_warmup_steps,\n","                            sentiment_only=FLAGS.sentiment_only,\n","                            commonsense_only=FLAGS.commonsense_only,\n","                            combination=FLAGS.combination,\n","                            )\n","\n","\n","params = {\n","    \"batch_size\": FLAGS.batch_size,\n","}\n","\n","estimator = tf.estimator.Estimator(model_fn=model_fn,\n","                                   config=run_config,\n","                                   params=params)\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Training\n","if FLAGS.do_train:\n","    train_file = os.path.join(FLAGS.data_dir, \"train.tf_record\")\n","    file_based_convert_examples_to_features(train_examples,\n","                                            label_list,\n","                                            FLAGS.max_seq_length,\n","                                            tokenizer,\n","                                            train_file)\n","\n","    tf.logging.info(\"***** Running training *****\")\n","    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n","    tf.logging.info(\"  Batch size = %d\", FLAGS.batch_size)\n","    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n","\n","    train_input_fn = file_based_input_fn_builder(input_file=train_file,\n","                                                 seq_length=FLAGS.max_seq_length,\n","                                                 is_training=True,\n","                                                 drop_remainder=False)\n","\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Evaluation\n","if FLAGS.do_eval:\n","    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n","\n","    eval_file = os.path.join(FLAGS.data_dir, \"eval.tf_record\")\n","    file_based_convert_examples_to_features(eval_examples,\n","                                            label_list,\n","                                            FLAGS.max_seq_length,\n","                                            tokenizer,\n","                                            eval_file)\n","\n","    tf.logging.info(\"***** Running evaluation *****\")\n","    tf.logging.info(\"  Num examples = %d\", len(eval_examples))\n","\n","    eval_steps = int(len(eval_examples) // FLAGS.batch_size)\n","\n","    eval_input_fn = file_based_input_fn_builder(input_file=eval_file,\n","                                                seq_length=FLAGS.max_seq_length,\n","                                                is_training=False,\n","                                                drop_remainder=False)\n","\n","    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","\n","    output_eval_file = os.path.join(FLAGS.results_dir, timestamp + \"eval_results.txt\")\n","    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n","        tf.logging.info(\"***** Eval results *****\")\n","        for key in sorted(result.keys()):\n","            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","# ---------------------------------------------------------------------------------------------------------------- #\n","# Test\n","if FLAGS.do_predict:\n","    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n","\n","    predict_file = os.path.join(FLAGS.data_dir, \"predict.tf_record\")\n","    file_based_convert_examples_to_features(predict_examples,\n","                                            label_list,\n","                                            FLAGS.max_seq_length,\n","                                            tokenizer,\n","                                            predict_file)\n","\n","    tf.logging.info(\"***** Running prediction*****\")\n","    tf.logging.info(\"  Num examples = %d\", len(predict_examples))\n","\n","    predict_input_fn = file_based_input_fn_builder(input_file=predict_file,\n","                                                   seq_length=FLAGS.max_seq_length,\n","                                                   is_training=False,\n","                                                   drop_remainder=False)\n","\n","    result = estimator.predict(input_fn=predict_input_fn)\n","\n","    output_predict_file = os.path.join(FLAGS.results_dir, timestamp + \"test_results.tsv\")\n","    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n","        num_written_lines = 0\n","        tf.logging.info(\"***** Predict results *****\")\n","        for (i, prediction) in enumerate(result):\n","            probabilities = prediction[\"probabilities\"]\n","            predict_label = str(prediction[\"predict_labels\"])\n","\n","            probs = [\"{0:.4f}\".format(class_probability) for class_probability in probabilities]\n","\n","            output_line = \"\\t\".join(probs) + \"\\t\" + predict_label + \"\\n\"\n","            writer.write(output_line)\n","            num_written_lines += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.309063 139882778761088 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': './output_G/', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3880898438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.967095 139882778761088 estimator.py:201] Using config: {'_model_dir': './output_G/', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3880898438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 3742\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.981432 139882778761088 bert_sct_utils_v2.py:322] Writing example 0 of 3742\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.985429 139882778761088 bert_sct_utils_v2.py:299] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:unique_id: train-fd958fb3-cf76-4f37-a129-ce53260cf24a\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.987985 139882778761088 bert_sct_utils_v2.py:300] unique_id: train-fd958fb3-cf76-4f37-a129-ce53260cf24a\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i drove my car through a bad area of town . at a stop ##light a man on the street corner began to cross . he walked up to my window and gestured for me to roll it down . he told me to remain on the smooth tip . [SEP] i was not sure what he meant so i sped off a moment later . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.991743 139882778761088 bert_sct_utils_v2.py:301] tokens: [CLS] i drove my car through a bad area of town . at a stop ##light a man on the street corner began to cross . he walked up to my window and gestured for me to roll it down . he told me to remain on the smooth tip . [SEP] i was not sure what he meant so i sped off a moment later . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 5225 2026 2482 2083 1037 2919 2181 1997 2237 1012 2012 1037 2644 7138 1037 2158 2006 1996 2395 3420 2211 2000 2892 1012 2002 2939 2039 2000 2026 3332 1998 11574 2005 2033 2000 4897 2009 2091 1012 2002 2409 2033 2000 3961 2006 1996 5744 5955 1012 102 1045 2001 2025 2469 2054 2002 3214 2061 1045 16887 2125 1037 2617 2101 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.993927 139882778761088 bert_sct_utils_v2.py:302] input_ids: 101 1045 5225 2026 2482 2083 1037 2919 2181 1997 2237 1012 2012 1037 2644 7138 1037 2158 2006 1996 2395 3420 2211 2000 2892 1012 2002 2939 2039 2000 2026 3332 1998 11574 2005 2033 2000 4897 2009 2091 1012 2002 2409 2033 2000 3961 2006 1996 5744 5955 1012 102 1045 2001 2025 2469 2054 2002 3214 2061 1045 16887 2125 1037 2617 2101 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.996404 139882778761088 bert_sct_utils_v2.py:303] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:48.998792 139882778761088 bert_sct_utils_v2.py:304] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.001167 139882778761088 bert_sct_utils_v2.py:305] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sentiment: (20,), [-0.5423  0.333   0.667   0.      0.      0.      1.      0.      0.\n","  0.      1.      0.      0.      0.      1.      0.     -0.2411  0.164\n","  0.836   0.    ]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.004997 139882778761088 bert_sct_utils_v2.py:306] sentiment: (20,), [-0.5423  0.333   0.667   0.      0.      0.      1.      0.      0.\n","  0.      1.      0.      0.      0.      1.      0.     -0.2411  0.164\n","  0.836   0.    ]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:cs_dist: (4,), [0.10817689 0.11406508 0.10792304 0.13321528]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.007230 139882778761088 bert_sct_utils_v2.py:307] cs_dist: (4,), [0.10817689 0.11406508 0.10792304 0.13321528]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.011380 139882778761088 bert_sct_utils_v2.py:299] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:unique_id: train-fd958fb3-cf76-4f37-a129-ce53260cf24a\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.013716 139882778761088 bert_sct_utils_v2.py:300] unique_id: train-fd958fb3-cf76-4f37-a129-ce53260cf24a\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i drove my car through a bad area of town . at a stop ##light a man on the street corner began to cross . he walked up to my window and gestured for me to roll it down . he told me to remain on the smooth tip . [SEP] michael tried to run away . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.016095 139882778761088 bert_sct_utils_v2.py:301] tokens: [CLS] i drove my car through a bad area of town . at a stop ##light a man on the street corner began to cross . he walked up to my window and gestured for me to roll it down . he told me to remain on the smooth tip . [SEP] michael tried to run away . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 5225 2026 2482 2083 1037 2919 2181 1997 2237 1012 2012 1037 2644 7138 1037 2158 2006 1996 2395 3420 2211 2000 2892 1012 2002 2939 2039 2000 2026 3332 1998 11574 2005 2033 2000 4897 2009 2091 1012 2002 2409 2033 2000 3961 2006 1996 5744 5955 1012 102 2745 2699 2000 2448 2185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.018280 139882778761088 bert_sct_utils_v2.py:302] input_ids: 101 1045 5225 2026 2482 2083 1037 2919 2181 1997 2237 1012 2012 1037 2644 7138 1037 2158 2006 1996 2395 3420 2211 2000 2892 1012 2002 2939 2039 2000 2026 3332 1998 11574 2005 2033 2000 4897 2009 2091 1012 2002 2409 2033 2000 3961 2006 1996 5744 5955 1012 102 2745 2699 2000 2448 2185 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.020781 139882778761088 bert_sct_utils_v2.py:303] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.023210 139882778761088 bert_sct_utils_v2.py:304] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.025589 139882778761088 bert_sct_utils_v2.py:305] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sentiment: (20,), [-0.5423  0.333   0.667   0.      0.      0.      1.      0.      0.\n","  0.      1.      0.      0.      0.      1.      0.      0.      0.\n","  1.      0.    ]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.028342 139882778761088 bert_sct_utils_v2.py:306] sentiment: (20,), [-0.5423  0.333   0.667   0.      0.      0.      1.      0.      0.\n","  0.      1.      0.      0.      0.      1.      0.      0.      0.\n","  1.      0.    ]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:cs_dist: (4,), [0.15442029 0.11819135 0.18656905 0.1276002 ]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.030943 139882778761088 bert_sct_utils_v2.py:307] cs_dist: (4,), [0.15442029 0.11819135 0.18656905 0.1276002 ]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.037312 139882778761088 bert_sct_utils_v2.py:299] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:unique_id: train-f1911438-d757-4798-953e-eee6c5397dc6\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.039639 139882778761088 bert_sct_utils_v2.py:300] unique_id: train-f1911438-d757-4798-953e-eee6c5397dc6\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] al and val were very close twins . they dressed alike , talked alike and were together 24 / 7 . one day , val was diagnosed with terminal cancer . she soon died , and al was completely devastated . [SEP] al felt even lone ##lier as time went on . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.042131 139882778761088 bert_sct_utils_v2.py:301] tokens: [CLS] al and val were very close twins . they dressed alike , talked alike and were together 24 / 7 . one day , val was diagnosed with terminal cancer . she soon died , and al was completely devastated . [SEP] al felt even lone ##lier as time went on . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2632 1998 11748 2020 2200 2485 8178 1012 2027 5102 11455 1010 5720 11455 1998 2020 2362 2484 1013 1021 1012 2028 2154 1010 11748 2001 11441 2007 5536 4456 1012 2016 2574 2351 1010 1998 2632 2001 3294 13879 1012 102 2632 2371 2130 10459 14355 2004 2051 2253 2006 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.044728 139882778761088 bert_sct_utils_v2.py:302] input_ids: 101 2632 1998 11748 2020 2200 2485 8178 1012 2027 5102 11455 1010 5720 11455 1998 2020 2362 2484 1013 1021 1012 2028 2154 1010 11748 2001 11441 2007 5536 4456 1012 2016 2574 2351 1010 1998 2632 2001 3294 13879 1012 102 2632 2371 2130 10459 14355 2004 2051 2253 2006 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.047529 139882778761088 bert_sct_utils_v2.py:303] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.050088 139882778761088 bert_sct_utils_v2.py:304] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.052551 139882778761088 bert_sct_utils_v2.py:305] label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sentiment: (20,), [ 0.      0.      1.      0.      0.      0.      1.      0.     -0.6597\n","  0.386   0.614   0.     -0.8357  0.568   0.432   0.     -0.34    0.255\n","  0.745   0.    ]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.055696 139882778761088 bert_sct_utils_v2.py:306] sentiment: (20,), [ 0.      0.      1.      0.      0.      0.      1.      0.     -0.6597\n","  0.386   0.614   0.     -0.8357  0.568   0.432   0.     -0.34    0.255\n","  0.745   0.    ]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:cs_dist: (4,), [0.05558484 0.14492406 0.13470303 0.13276787]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.058429 139882778761088 bert_sct_utils_v2.py:307] cs_dist: (4,), [0.05558484 0.14492406 0.13470303 0.13276787]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.062195 139882778761088 bert_sct_utils_v2.py:299] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:unique_id: train-f1911438-d757-4798-953e-eee6c5397dc6\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.065160 139882778761088 bert_sct_utils_v2.py:300] unique_id: train-f1911438-d757-4798-953e-eee6c5397dc6\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] al and val were very close twins . they dressed alike , talked alike and were together 24 / 7 . one day , val was diagnosed with terminal cancer . she soon died , and al was completely devastated . [SEP] al and val went out to eat . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.067675 139882778761088 bert_sct_utils_v2.py:301] tokens: [CLS] al and val were very close twins . they dressed alike , talked alike and were together 24 / 7 . one day , val was diagnosed with terminal cancer . she soon died , and al was completely devastated . [SEP] al and val went out to eat . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2632 1998 11748 2020 2200 2485 8178 1012 2027 5102 11455 1010 5720 11455 1998 2020 2362 2484 1013 1021 1012 2028 2154 1010 11748 2001 11441 2007 5536 4456 1012 2016 2574 2351 1010 1998 2632 2001 3294 13879 1012 102 2632 1998 11748 2253 2041 2000 4521 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.069834 139882778761088 bert_sct_utils_v2.py:302] input_ids: 101 2632 1998 11748 2020 2200 2485 8178 1012 2027 5102 11455 1010 5720 11455 1998 2020 2362 2484 1013 1021 1012 2028 2154 1010 11748 2001 11441 2007 5536 4456 1012 2016 2574 2351 1010 1998 2632 2001 3294 13879 1012 102 2632 1998 11748 2253 2041 2000 4521 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.072419 139882778761088 bert_sct_utils_v2.py:303] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.075446 139882778761088 bert_sct_utils_v2.py:304] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.077925 139882778761088 bert_sct_utils_v2.py:305] label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:sentiment: (20,), [ 0.      0.      1.      0.      0.      0.      1.      0.     -0.6597\n","  0.386   0.614   0.     -0.8357  0.568   0.432   0.      0.      0.\n","  1.      0.    ]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.080887 139882778761088 bert_sct_utils_v2.py:306] sentiment: (20,), [ 0.      0.      1.      0.      0.      0.      1.      0.     -0.6597\n","  0.386   0.614   0.     -0.8357  0.568   0.432   0.      0.      0.\n","  1.      0.    ]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:cs_dist: (4,), [0.03383938 0.07851694 0.04358886 0.05521765]\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:49.083585 139882778761088 bert_sct_utils_v2.py:307] cs_dist: (4,), [0.03383938 0.07851694 0.04358886 0.05521765]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 1000 of 3742\n"],"name":"stdout"},{"output_type":"stream","text":["I0611 12:25:51.717880 139882778761088 bert_sct_utils_v2.py:322] Writing example 1000 of 3742\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nZR1wlmir_QO","colab_type":"code","colab":{}},"source":["import shutil\n","shutil.rmtree(\"./output_G\")\n","\n","\n","# shutil.move(\"./output/\", \"./output_A/\")\n","# shutil.move(\"./results/\", \"./results_A/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUBgfsm7ictb","colab_type":"code","colab":{}},"source":["FLAGS.do_train = True\n","FLAGS.num_train_epochs = 3.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEed72YQsLJr","colab_type":"code","colab":{}},"source":["# FLAGS.data_dir = \"./gdrive/My Drive/NLU/Projects/project_2/data_pp_test/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ry8rj7rh3_QA","colab_type":"code","colab":{}},"source":["# shutil.move(\"./results_D/\", \"./gdrive/My Drive/NLU/Projects/project_2/results/results_D/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLmsUV6h_vxp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}